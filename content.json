[{"title":"Hibernate缓存 查询缓存","date":"2017-03-30T05:58:14.000Z","path":"2017/03/30/hibernate/Hibernate缓存_查询缓存/","text":"网上说query.setCacheable(true)或criteria.setCacheable(true)`` 这两种方式的缓存命中率低，个人认为谈论这个“无卵用”； 我在测试的时候发现，上面的操作会受配置的限制，必须在配置文件中打开hibernate.cache.use_query_cache=true，之后setCacheable`才起作用； 查询缓存可以解决二级缓存的不足；它的作用范围也是SessionFactory； 可以缓存hql语句查询，也可以缓存query和criteria查询； 下面针对query和criteria进行测试：1234567891011121314151617181920212223242526/** * 对查询缓存测试 &lt;br&gt; * 1. 只有B处起作用，作用于session; &lt;br&gt; * 2. 配置文件中打开query_cache的前提下，setCacheable 是起作用的 */@Testpublic void testCriteriaInCache() &#123; System.out.println(&quot;=============&quot;); Session session = hibernateTemplate.getSessionFactory().openSession(); Criteria criteria = session.createCriteria(UserModel.class); criteria.setCacheable(true); // 这里仅对B处起作用 criteria.add(Restrictions.eq(&quot;name&quot;, &quot;Sucre&quot;)); System.out.println(&quot;=============A&quot; + criteria.list()); System.out.println(&quot;=============B&quot; + criteria.list()); // B criteria.add(Restrictions.eq(&quot;id&quot;, 1)); System.out.println(&quot;=============C&quot; + criteria.list()); criteria = session.createCriteria(UserModel.class); criteria.setCacheable(true); criteria.add(Restrictions.eq(&quot;id&quot;, 1)); System.out.println(&quot;=============D&quot; + criteria.list()); session = hibernateTemplate.getSessionFactory().openSession(); criteria = session.createCriteria(UserModel.class); criteria.setCacheable(true); criteria.add(Restrictions.eq(&quot;id&quot;, 1)); System.out.println(&quot;=============E&quot; + criteria.list());&#125; 1234567891011121314151617181920212223242526272829/** * 测试查询缓存 &lt;br&gt; * 1. BC两处都是使用的A产生的缓存，作用于SessionFactory &lt;br&gt; * 2. 配置文件中打开query_cache的前提下，setCacheable 是起作用的 */@Testpublic void testQueryInCache() &#123; System.out.println(&quot;=============&quot;); Session session = hibernateTemplate.getSessionFactory().openSession(); Query query = session .createQuery(&quot;select u.name from UserModel as u where u.id=?&quot;); query.setParameter(0, 1); query.setCacheable(true); String name = (String) query.list().get(0); System.out.println(&quot;=============A&quot; + name); // A query = session .createQuery(&quot;select u.name from UserModel as u where u.id=?&quot;); query.setParameter(0, 1); query.setCacheable(true); name = (String) query.list().get(0); System.out.println(&quot;=============B&quot; + name); // B session = hibernateTemplate.getSessionFactory().openSession(); query = session .createQuery(&quot;select u.name from UserModel as u where u.id=?&quot;); query.setParameter(0, 1); query.setCacheable(true); name = (String) query.list().get(0); System.out.println(&quot;=============C&quot; + name); // C&#125;","tags":[{"name":"hibernate","slug":"hibernate","permalink":"http://jishusuishouji.github.io/tags/hibernate/"}]},{"title":"hibernate的查询缓存","date":"2017-03-30T05:47:52.000Z","path":"2017/03/30/hibernate/hibernate的查询缓存/","text":"hibernate的查询缓存主要是针对普通属性结果集的缓存，而对于实体对象的结果集只缓存id。在一级缓存,二级缓存和查询缓存都打开的情况下做查询操作时这样的：查询普通属性，会先到查询缓存中取，如果没有，则查询数据库；查询实体，会先到查询缓存中取id，如果有，则根据id到缓存(一级/二级)中取实体，如果缓存中取不到实体，再查询数据库。 和一级/二级缓存不同，查询缓存的生命周期是不确定的，当前关联的表发生改变时，查询缓存的生命周期结束。查询缓存的配置和使用也是很简单的：1&gt;查询缓存的启用不但要在配置文件中进行配置1&lt;property name=&quot;hibernate.cache.use_query_cache&quot;&gt;true&lt;/property&gt; 2&gt;还要在程序中显示的进行启用1query.setCacheable(true); 1&gt;查询缓存的启用不但要在配置文件中进行配置 ——-换成spring配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;bean id=&quot;propertyConfigurer&quot; class=&quot;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer&quot;&gt; &lt;property name=&quot;locations&quot;&gt; &lt;list&gt; &lt;value&gt;/WEB-INF/config/jdbc.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=&quot;dataSource&quot; class=&quot;com.mchange.v2.c3p0.ComboPooledDataSource&quot;&gt; &lt;property name=&quot;driverClass&quot; value=&quot;$&#123;jdbc.driverClassName&#125;&quot; /&gt; &lt;property name=&quot;jdbcUrl&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt; &lt;property name=&quot;user&quot; value=&quot;$&#123;jdbc.username&#125;&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt; &lt;property name=&quot;autoCommitOnClose&quot; value=&quot;true&quot;/&gt; &lt;property name=&quot;checkoutTimeout&quot; value=&quot;$&#123;cpool.checkoutTimeout&#125;&quot;/&gt; &lt;property name=&quot;initialPoolSize&quot; value=&quot;$&#123;cpool.minPoolSize&#125;&quot;/&gt; &lt;property name=&quot;minPoolSize&quot; value=&quot;$&#123;cpool.minPoolSize&#125;&quot;/&gt; &lt;property name=&quot;maxPoolSize&quot; value=&quot;$&#123;cpool.maxPoolSize&#125;&quot;/&gt; &lt;property name=&quot;maxIdleTime&quot; value=&quot;$&#123;cpool.maxIdleTime&#125;&quot;/&gt; &lt;property name=&quot;acquireIncrement&quot; value=&quot;$&#123;cpool.acquireIncrement&#125;&quot;/&gt; &lt;property name=&quot;maxIdleTimeExcessConnections&quot; value=&quot;$&#123;cpool.maxIdleTimeExcessConnections&#125;&quot;/&gt;&lt;/bean&gt;&lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3.LocalSessionFactoryBean&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;dataSource&quot;/&gt; &lt;property name=&quot;mappingLocations&quot;&gt; &lt;list&gt; &lt;value&gt;classpath*:/com/jeecms/core/entity/hbm/*.hbm.xml&lt;/value&gt; &lt;value&gt;classpath*:/com/jeecms/cms/entity/main/hbm/*.hbm.xml&lt;/value&gt; &lt;value&gt;classpath*:/com/jeecms/cms/entity/assist/hbm/*.hbm.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;hibernateProperties&quot;&gt; &lt;value&gt; hibernate.dialect=org.hibernate.dialect.MySQLInnoDBDialect hibernate.show_sql=false hibernate.format_sql=false hibernate.query.substitutions=true 1, false 0 hibernate.jdbc.batch_size=20 //查询缓存配置 hibernate.cache.use_query_cache=true &lt;/value&gt; &lt;/property&gt; &lt;property name=&quot;entityInterceptor&quot;&gt; &lt;ref local=&quot;treeInterceptor&quot;/&gt; &lt;/property&gt; &lt;property name=&quot;cacheProvider&quot;&gt; &lt;ref local=&quot;cacheProvider&quot;/&gt; &lt;/property&gt; &lt;property name=&quot;lobHandler&quot;&gt; &lt;ref bean=&quot;lobHandler&quot; /&gt; &lt;/property&gt;&lt;/bean&gt; 2&gt;还要在程序中显示的进行启用1234public List&lt;CmsSite&gt; getList(boolean cacheable) &#123; String hql = &quot;from CmsSite bean order by bean.id asc&quot;; return getSession().createQuery(hql).setCacheable(cacheable).list(); &#125; 1.实体类：public class Student { private Integer id; private String name; //一系列的setter.getter方法 } ##2.映射文件 Student.hbm.xml1234567891011&lt;class name=&quot;com.sxt.hibernate.cache.entity.Student&quot; table=&quot;sxt_hibernate_student&quot;&gt; &lt;!-- 指定本类的对象使用二级缓存(这也可以放在hibernate.cfg.xml中统一指定) --&gt; &lt;!-- &lt;cache usage=&quot;read-only&quot;/&gt; --&gt; &lt;id name=&quot;id&quot; length=&quot;4&quot;&gt; &lt;generator class=&quot;native&quot;&gt;&lt;/generator&gt; &lt;/id&gt; &lt;property name=&quot;name&quot; length=&quot;10&quot;&gt;&lt;/property&gt; &lt;/class&gt; 3.hibernate配置文件：hibernate.cfg.xml &lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;property name=&quot;hibernate.connection.url&quot;&gt;jdbc:oracle:thin:@localhost:1521:ORCL10&lt;/property&gt; &lt;property name=&quot;hibernate.connection.driver_class&quot;&gt;oracle.jdbc.driver.OracleDriver&lt;/property&gt; &lt;property name=&quot;hibernate.connection.username&quot;&gt;scott&lt;/property&gt; &lt;property name=&quot;hibernate.connection.password&quot;&gt;yf123&lt;/property&gt; &lt;property name=&quot;hibernate.dialect&quot;&gt;org.hibernate.dialect.Oracle9Dialect&lt;/property&gt; &lt;property name=&quot;hibernate.show_sql&quot;&gt;true&lt;/property&gt; &lt;!-- 开启二级缓存,其实hibernate默认就是开启的,这里显示的指定一下 --&gt; &lt;property name=&quot;hibernate.cache.use_second_level_cache&quot;&gt;true&lt;/property&gt; &lt;!-- 指定二级缓存产品的提供商 --&gt; &lt;property name=&quot;hibernate.cache.provider_class&quot;&gt;org.hibernate.cache.EhCacheProvider&lt;/property&gt; &lt;!-- 启用查询缓存 --&gt; &lt;property name=&quot;hibernate.cache.use_query_cache&quot;&gt;true&lt;/property&gt; &lt;mapping resource=&quot;com/sxt/hibernate/cache/entity/Student.hbm.xml&quot;/&gt; &lt;!-- 指定那些类使用二级缓存 --&gt; &lt;class-cache usage=&quot;read-only&quot; class=&quot;com.sxt.hibernate.cache.entity.Student&quot;/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; 4.测试方法：1234567891011121314151617181920212223242526272829303132333435363738public static void main(String[] args) &#123; Session session = null; Transaction t = null; *//** * 开启查询缓存,关闭二级缓存, 开启一个session,分别调用query.list */ //如果不用查询缓存的话,那两个都发出查询语句,这也是默认的情况. /* try &#123; session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s.name from Student s&quot;); //启用查询缓存 query.setCacheable(true); List&lt;String&gt; names = query.list(); for (Iterator&lt;String&gt; it = names.iterator(); it.hasNext();) &#123; String name = it.next(); System.out.println(name); &#125; System.out.println(&quot;================================&quot;); query = session.createQuery(&quot;select s.name from Student s&quot;); //启用查询缓存 query.setCacheable(true); //没有发出查询语句,因为这里使用的查询缓存 names = query.list(); for (Iterator&lt;String&gt; it = names.iterator(); it.hasNext();) &#123; String name = it.next(); System.out.println(name); &#125; t.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); t.rollback(); &#125; finally &#123; HibernateUtils.closeSession(session); &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@SuppressWarnings(&quot;unchecked&quot;) public static void main(String[] args) &#123; Session session = null; Transaction t = null; *//** * 开启查询缓存,关闭二级缓存, 开启两个session,分别调用query.list *//* //如果不用查询缓存的话,那两个都发出查询语句,这也是默认的情况. try &#123; session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s.name from Student s&quot;); //启用查询缓存 //query.setCacheable(true); List&lt;String&gt; names = query.list(); for (Iterator&lt;String&gt; it = names.iterator(); it.hasNext();) &#123; String name = it.next(); System.out.println(name); &#125; t.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); t.rollback(); &#125; finally &#123; HibernateUtils.closeSession(session); &#125; System.out.println(&quot;================================&quot;); try &#123; session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s.name from Student s&quot;); //启用查询缓存 //query.setCacheable(true); //不会发出查询语句,因为查询缓存和session无关. List&lt;String&gt; names = query.list(); for (Iterator&lt;String&gt; it = names.iterator(); it.hasNext();) &#123; String name = it.next(); System.out.println(name); &#125; t.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); t.rollback(); &#125; finally &#123; HibernateUtils.closeSession(session); &#125; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@SuppressWarnings(&quot;unchecked&quot;) public static void main(String[] args) &#123; Session session = null; Transaction t = null; *//** * 开启查询缓存,关闭二级缓存, 开启两个session,分别调用query.iterate *//* //如果不用查询缓存的话,那两个都发出查询语句,这也是默认的情况. try &#123; session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s.name from Student s&quot;); //启用查询缓存 query.setCacheable(true); for (Iterator&lt;String&gt; it = query.iterate(); it.hasNext();) &#123; String name = it.next(); System.out.println(name); &#125; t.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); t.rollback(); &#125; finally &#123; HibernateUtils.closeSession(session); &#125; System.out.println(&quot;================================&quot;); try &#123; session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s.name from Student s&quot;); //启用查询缓存 query.setCacheable(true); //会发出查询语句,因为query.iterate不使用查询缓存 for (Iterator&lt;String&gt; it = query.iterate(); it.hasNext();) &#123; String name = it.next(); System.out.println(name); &#125; t.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); t.rollback(); &#125; finally &#123; HibernateUtils.closeSession(session); &#125; &#125; ``` @SuppressWarnings(“unchecked”) public static void main(String[] args) { Session session = null; Transaction t = null; *//** * 关闭查询缓存,关闭二级缓存, 开启两个session,分别调用query.list查询实体对象 *//* //如果不用查询缓存的话,那两个都发出查询语句,这也是默认的情况. try { session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s from Student s&quot;); //启用查询缓存 //query.setCacheable(true); List&lt;Student&gt; students = query.list(); for (Iterator&lt;Student&gt; it = students.iterator(); it.hasNext();) { Student s = it.next(); System.out.println(s.getName()); } t.commit(); } catch (Exception e) { e.printStackTrace(); t.rollback(); } finally { HibernateUtils.closeSession(session); } System.out.println(&quot;================================&quot;); try { session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s from Student s&quot;); //启用查询缓存 //query.setCacheable(true); //会发出查询语句,因为list默认每次都会发出sql语句 List&lt;Student&gt; students = query.list(); for (Iterator&lt;Student&gt; it = students.iterator(); it.hasNext();) { Student s = it.next(); System.out.println(s.getName()); } t.commit(); } catch (Exception e) { e.printStackTrace(); t.rollback(); } finally { HibernateUtils.closeSession(session); } }*/ /* @SuppressWarnings(“unchecked”) public static void main(String[] args) { Session session = null; Transaction t = null; *//** * 开启查询缓存,关闭二级缓存, 开启两个session,分别调用query.list查询实体对象 *//* //如果不用查询缓存的话,那两个都发出查询语句,这也是默认的情况. try { session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s from Student s&quot;); //启用查询缓存 query.setCacheable(true); List&lt;Student&gt; students = query.list(); for (Iterator&lt;Student&gt; it = students.iterator(); it.hasNext();) { Student s = it.next(); System.out.println(s.getName()); } t.commit(); } catch (Exception e) { e.printStackTrace(); t.rollback(); } finally { HibernateUtils.closeSession(session); } System.out.println(&quot;================================&quot;); try { session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s from Student s&quot;); //启用查询缓存 query.setCacheable(true); //会发出根据id查询实体的n条查询语句,因为这种情况下,查询过程是这样的： // 在第一次执行list时,会把查询对象的id缓存到查询缓存里 // 第二次执行list时, 会遍历查询缓存里的id到缓存里去找实体对象,由于这里没找到实体对象, //所以就发出n条查询语句到数据库中查询. List&lt;Student&gt; students = query.list(); for (Iterator&lt;Student&gt; it = students.iterator(); it.hasNext();) { Student s = it.next(); System.out.println(s.getName()); } t.commit(); } catch (Exception e) { e.printStackTrace(); t.rollback(); } finally { HibernateUtils.closeSession(session); } }*/ @SuppressWarnings(“unchecked”) public static void main(String[] args) { Session session = null; Transaction t = null; /** * 开启查询缓存,开启二级缓存, 开启两个session,分别调用query.list查询实体对象 */ //如果不用查询缓存的话,那两个都发出查询语句,这也是默认的情况. try { session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s from Student s&quot;); //启用查询缓存 query.setCacheable(true); List&lt;Student&gt; students = query.list(); for (Iterator&lt;Student&gt; it = students.iterator(); it.hasNext();) { Student s = it.next(); System.out.println(s.getName()); } t.commit(); } catch (Exception e) { e.printStackTrace(); t.rollback(); } finally { HibernateUtils.closeSession(session); } System.out.println(&quot;================================&quot;); try { session = HibernateUtils.getSession(); t = session.beginTransaction(); Query query = session.createQuery(&quot;select s from Student s&quot;); //启用查询缓存 query.setCacheable(true); //不会发出查询语句,因为这种情况下,查询过程是这样的： // 在第一次执行list时,会把查询对象的id缓存到查询缓存里 // 第二次执行list时, 会遍历查询缓存里的id到缓存里去找实体对象,由于这里开启了二级缓存,可以找到目标实体对象, //所以就不会再发出n条查询语句. List&lt;Student&gt; students = query.list(); for (Iterator&lt;Student&gt; it = students.iterator(); it.hasNext();) { Student s = it.next(); System.out.println(s.getName()); } t.commit(); } catch (Exception e) { e.printStackTrace(); t.rollback(); } finally { HibernateUtils.closeSession(session); } }","tags":[{"name":"hibernate","slug":"hibernate","permalink":"http://jishusuishouji.github.io/tags/hibernate/"}]},{"title":"Layout of Log4j","date":"2017-03-28T07:02:00.000Z","path":"2017/03/28/Log4j/Layout_of_Log4j/","text":"本文档使用Log4j版本为1.2.17 1. Layout介绍Log4j Layout主要用来控制日志的序列化格式，比如时间、线程号、日志消息对齐方式等，是log4j体系结构中的核心组成部分之一。 Layout抽象类声明为:1public abstract class Layout implements OptionHandler Layout实现了OptionHandler接口，OptionHandler仅包含一个方法activateOptions()。对实现了OptionHandler接口的模块，调用属性setter方法后，log4j的配置器类会调用此模块的activateOptions实现以激活配置。OptionHandler存在的原因是有些属性彼此依赖，在它们在全部加载完成之前是无法激活的，这个方法用于在模块变为激活和就绪之前用来执行任何必要任务的机制。比如： 某模块有字符串属性fileName属性，表示log4j用户配置的写出日志文件名，使用前需要创建File对象获取文件写出IO流，具体则是由activateOptions完成文件的打开等，具体可见log4j的FileAppender实现中对文件名和文件IO的操作。 Layout类的方法或接口如下，abstract修饰的需要具体子类实现: //abstract修饰需要具体子类实现，将日志事件渲染为待打印的日志文本字符串，可写出到Appenderabstract public String format(LoggingEvent event )//format函数返回的格式化文本类型，默认返回为”text/plain”public String getContentType()//针对HTMLLayout类的格式化输出，html字符串的头部，默认nullpublic String getHeader()//针对HTMLLayout类的格式化输出，html字符串的尾部，默认nullpublic String getFooter()//对于LayoutEvent中异常的处理模式，true表示忽略异常，异常会到达Appender，由Appender负责渲染为打印持久化字符串信息；false表示由Layout负责渲染异常信息。SimpleLayout、TTCCLayout、PatternLayout实现返回true;XMLLayout实现返回false，由Appender处理渲染异常消息。abstract public boolean ignoresThrowable()Layout是对序列化每一次LoggingEvent的抽象，核心是format方法，format作为抽象方法，由具体子类实现具体的序列化方式。具体子类有： SimpleLayoutTTCCLayoutPatternLayoutXMLLayoutHTMLLayoutDateLayoutLayout继承体系 XMLLayout XMLLayout实现了根据log4j.dtd序列化输出xml格式的日志文本，默认的log4j.dtd文件在/org/apache/log4j/xml/log4j.dtd目录下，注意，XMLLayout打印输出的并非完整XML文件，并不包括&lt;?xml version=”1.0” ?&gt;等XML头部，XMLLayout的目的是输出XML的部分片段，应用可将此片段整合嵌入到其它XML文件中。XMLLayout有成员属性：locationInfo表示是否打印位置信息，即日志事件发生的代码文件名、日志记录点代码行号等信息，log4j配置文件中需要配置为LocationInfoproperties表示是否打印MDC中的Key-Value信息，默认为false，log4j配置文件中需要配置为Properties 注意：log4j的各个模块涉及的成员属性时，如果属性有set方法，一般表示此属性可通过log4j.properties进行配置，具体配置属性值为属性的setXXX方法去掉set前缀。 示例如上面的locationInfo和properties配置: log4j.appender.Console.layout.LocationInfo=truelog4j.appender.Console.layout.Properties=true XMLLayout继承自Layout的方法实现有： //配置激活的接口实现，来自于OptionHandler interface，方法体为空public void activateOptions()//返回false，表示XMLLayout自己处理异常信息public boolean ignoresThrowable()public String format( final LoggingEvent event)2.1 format实现 format按照日志message、NDC、getThrowableStrRep、日志位置信息、MDC的顺序，并按照XML格式序列化LoggingEvent。log4j实现时使用StringBuffer避免字符串拼接的开销（JAVA中String是不可变类），具体使用时设置了StringBuffer的默认长度即DEFAULT_SIZE = 256，最大长度UPPER_LIMIT = 2048。每次format函数调用时，如果当前StringBuffer容量未超过上限，则复用已有的StringBuffer并清空已有内容；如果当前StringBuffer容量超过UPPER_LIMIT上限，则创建一个新的StringBuffer将当前LoggingEvent 序列化到其中，目的是尽量减少内存的占用量。 if(buf.capacity() &gt; UPPER_LIMIT) { buf = new StringBuffer(DEFAULT_SIZE);} else { buf.setLength(0);}xml序列化中，对于属性如 timestamp=”1452874282177” level=“INFO”，为了保持生成的文本符合XML语法，需要对特殊字符进行转义处理。对于属性使用org.apache.log4j.helpers.Transform.escapeTags做转义。对于文本子元素如 &lt;![CDATA[123]]&gt;，使用org.apache.log4j.helpers.Transform.appendEscapingCDATA做转义，将message放在 &lt;![CDATA[ 和 ]] 之间，避免文本破坏XML语法。 处理的XML特殊字符有（简单字符串替换）: -&gt; &gt;&lt; -&gt; &lt;&amp; -&gt; &amp;“ -&gt; &quot;2.2 demo demo java code: Logger logger = Logger.getLogger(LayoutTest.class);NDC.push(“ndc message”);logger.info(“info:123”);logger.warn(“warn:abc”);logger.info(“exception”, new RuntimeException(“run time exception”));demo log4j config: log4j.rootLogger=INFO,Consolelog4j.appender.Console=org.apache.log4j.ConsoleAppenderlog4j.appender.Console.target=System.outlog4j.appender.Console.layout=org.apache.log4j.xml.XMLLayoutlog4j.appender.Console.layout.LocationInfo=truelog4j.appender.Console.layout.Properties=true demo 日志输出: &lt;![CDATA[info:123]]&gt; &lt;![CDATA[ndc message]]&gt; &lt;![CDATA[warn:abc]]&gt; &lt;![CDATA[ndc message]]&gt; &lt;![CDATA[error:xyz]]&gt; &lt;![CDATA[ndc message]]&gt; &lt;![CDATA[exception]]&gt; &lt;![CDATA[ndc message]]&gt; &lt;![CDATA[java.lang.RuntimeException: run time exceptionat com.luohw.log4j.LayoutTest.test(LayoutTest.java:16)at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)… …at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:382)at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:192)]]&gt; HTMLLayout HTMLLayout用于将每次的LoggingEvent序列化为HTML格式字符串，具体的内容组织为html的表格。生成的HTML文本为完整的一份HTML格式代码（不同于XMLLayout的部分片段），包含html、head、body、具体table信息等。一份HTML日志文档可以包含多条LoggingEvent序列化输出，但header和footer只会输出一次（具体是有Appender打开和关闭相关流时输出）。 注意：如果有Appender使用HTMLLayout，需要设置Appender的字符编码为UTF-8 或者 UTF-16，否则非ASCII字符会产生乱码。 locationInfo表示是否打印位置信息，即日志事件发生的代码文件名、代码行号，log4j配置文件中需要配置为LocationInfo title输出html文档head的title部分，默认为Log4J Log Messages，log4j配置文件中需要配置为Title XMLLayout继承自Layout的方法实现有： //默认返回”text/html”public String getContentType()//配置加载完成后操作，实现为空public void activateOptions()//返回相应HTML头部部分，具体是html、head、title以及body、table的开始部分public String getHeader()//返回相应html尾部，具体是table、body、html的html闭合标签public String getFooter()//返回false，即由HTMLLayout本身处理异常信息格式化，HTMLLayout有成员函数appendThrowableAsHTML，具体是将Throwable对应的字符串做相关转移和替换处理，以符合html语法public boolean ignoresThrowable()//具体序列化LoggingEvent为字符串public String format(LoggingEvent event)3.1 format实现 a. 缓冲StringBuffer更新，判断容量是否超过HTMLLayout的MAX_CAPACITY(1024)，如果超过则创建新的StringBuffer，否则复用原有的StringBuffer，避免内存浪费，具体和XMLLayout原理一致。b. 输出时间、线程、Level等上下文信息，根据locationInfo（如果locationInfo为true）、Level等具体字符串拼接和格式化 0mainINFOcom.luohw.log4j.LayoutTestLayoutTest.java:12info:123 c. 输出NDC信息 NDC: ndc message d. 如果有则输出异常栈信息，一般Logger的日志函数info、warn、error等都有带Throwable型参的重载e. 没有MDC相关信息的格式化输出 3.2 demo demo java code: Logger logger = Logger.getLogger(LayoutTest.class);NDC.push(“ndc message”);logger.info(“info:123”);logger.warn(“warn:abc”);logger.error(“error:xyz”);logger.info(“exception”, new RuntimeException(“run time exception”));demo log4j config: log4j.rootLogger=INFO,Consolelog4j.appender.Console=org.apache.log4j.ConsoleAppenderlog4j.appender.Console.target=System.outlog4j.appender.Console.layout=org.apache.log4j.HTMLLayoutlog4j.appender.Console.layout.LocationInfo=truelog4j.appender.Console.layout.Title=luohw@log4j demo浏览器打开日志输出html: html see more …","tags":[{"name":"Log4j","slug":"Log4j","permalink":"http://jishusuishouji.github.io/tags/Log4j/"}]},{"title":"MongoDB两阶段提交实现事务","date":"2017-03-27T13:12:46.000Z","path":"2017/03/27/mongodb/MongoDB两阶段提交实现事务/","text":"MongoDB数据库中操作单个文档总是原子性的，然而，涉及多个文档的操作，通常被作为一个“事务”，而不是原子性的。因为文档可以是相当复杂并且包含多个嵌套文档，单文档的原子性对许多实际用例提供了支持。尽管单文档操作是原子性的，在某些情况下，需要多文档事务。在这些情况下，使用两阶段提交，提供这些类型的多文档更新支持。因为文档可以表示为Pending数据和状态，可以使用一个两阶段提交确保数据是一致的，在一个错误的情况下，事务前的状态是可恢复的。 事务最常见的例子是以可靠的方式从A账户转账到B账户，在关系型数据库中，此操作将从A账户减掉金额和给B账户增加金额的操作封装在单个原子事务中。在MongoDB中，可以使用两阶段提交达到相同的效果。本文中的所有示例使用mongo shell与数据库进行交互,并假设有两个集合：首先，一个名为accounts的集合存储每个账户的文档数据，另一个名为transactions的集合存储事务本身。 首先创建两个名为A和B的账户，使用下面的命令：12db.accounts.save(&#123;name: &quot;A&quot;, balance: 1000, pendingTransactions: []&#125;)db.accounts.save(&#123;name: &quot;B&quot;, balance: 1000, pendingTransactions: []&#125;) 使用find()方法验证这两个操作已经成功：1db.accounts.find() mongo会返回两个类似下面的文档：12&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc66cb8a04f512696151f&quot;), &quot;name&quot; : &quot;A&quot;, &quot;balance&quot; : 1000, &quot;pendingTransactions&quot; : [ ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc67bb8a04f5126961520&quot;), &quot;name&quot; : &quot;B&quot;, &quot;balance&quot; : 1000, &quot;pendingTransactions&quot; : [ ] &#125; 事务过程：设置事务初始状态initial： 通过插入下面的文档创建transaction集合，transaction文档持有源(source)和目标(destination)，它们引用自accounts集合文档的字段名，以及value字段表示改变balance字段数量的数据。最后，state字段反映事务的当前状态。1db.transactions.save(&#123;source: &quot;A&quot;, destination: &quot;B&quot;, value: 100, state: &quot;initial&quot;&#125;) 验证这个操作已经成功，使用find()：1db.transactions.find() 这个操作会返回一个类似下面的文档：1&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;), &quot;source&quot; : &quot;A&quot;, &quot;destination&quot; : &quot;B&quot;, &quot;value&quot; : 100, &quot;state&quot; : &quot;initial&quot; &#125; 切换事务到Pending状态：在修改accounts集合记录之前，将事务状态从initial设置为pending。使用findOne()方法将transaction文档赋值给shell会话中的局部变量t：1t = db.transactions.findOne(&#123;state: &quot;initial&quot;&#125;) 变量t创建后，shell将返回它的值，将会看到如下的输出：1&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;), &quot;source&quot; : &quot;A&quot;, &quot;destination&quot; : &quot;B&quot;, &quot;value&quot; : 100, &quot;state&quot; : &quot;initial&quot; &#125; 使用update()改变state的值为pending：12db.transactions.update(&#123;_id: t._id&#125;, &#123;$set: &#123;state: &quot;pending&quot;&#125;&#125;)db.transactions.find() find()操作将返回transaction集合的内容，类似下面：1&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;), &quot;source&quot; : &quot;A&quot;, &quot;destination&quot; : &quot;B&quot;, &quot;value&quot; : 100, &quot;state&quot; : &quot;pending&quot; &#125; 将事务应用到两个账户：使用update()方法应用事务到两个账户。在update()查询中，条件pendingTransactions:{$ne:t._id}阻止事务更新账户，如果账户的pendingTransaction字段包含事务t的_id：12345678db.accounts.update( &#123; name: t.source, pendingTransactions: &#123; $ne: t._id &#125; &#125;, &#123; $inc: &#123; balance: -t.value &#125;, $push: &#123; pendingTransactions: t._id &#125; &#125;)db.accounts.update( &#123; name: t.destination, pendingTransactions: &#123; $ne: t._id &#125; &#125;, &#123; $inc: &#123; balance: t.value &#125;, $push: &#123; pendingTransactions: t._id &#125; &#125;) 1db.accounts.find() find()操作将返回accounts集合的内容，现在应该类似于下面的内容：12&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc97fb8a04f5126961523&quot;), &quot;balance&quot; : 900, &quot;name&quot; : &quot;A&quot;, &quot;pendingTransactions&quot; : [ ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;) ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc984b8a04f5126961524&quot;), &quot;balance&quot; : 1100, &quot;name&quot; : &quot;B&quot;, &quot;pendingTransactions&quot; : [ ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;) ] &#125; 设置事务状态为committed：使用下面的update()操作设置事务的状态为committed：12db.transactions.update(&#123;_id: t._id&#125;, &#123;$set: &#123;state: &quot;committed&quot;&#125;&#125;)db.transactions.find() find()操作发回transactions集合的内容，现在应该类似下面的内容：1&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;), &quot;destination&quot; : &quot;B&quot;, &quot;source&quot; : &quot;A&quot;, &quot;state&quot; : &quot;committed&quot;, &quot;value&quot; : 100 &#125; 移除pending事务：使用下面的update()操作从accounts集合中移除pending事务：123db.accounts.update(&#123;name: t.source&#125;, &#123;$pull: &#123;pendingTransactions: t._id&#125;&#125;)db.accounts.update(&#123;name: t.destination&#125;, &#123;$pull: &#123;pendingTransactions: t._id&#125;&#125;)db.accounts.find() find()操作返回accounts集合内容，现在应该类似下面内容： 12&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc97fb8a04f5126961523&quot;), &quot;balance&quot; : 900, &quot;name&quot; : &quot;A&quot;, &quot;pendingTransactions&quot; : [ ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc984b8a04f5126961524&quot;), &quot;balance&quot; : 1100, &quot;name&quot; : &quot;B&quot;, &quot;pendingTransactions&quot; : [ ] &#125; 设置事务状态为done：通过设置transaction文档的state为done完成事务：12db.transactions.update(&#123;_id: t._id&#125;, &#123;$set: &#123;state: &quot;done&quot;&#125;&#125;)db.transactions.find() find()操作返回transaction集合的内容，此时应该类似下面：1&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc7a8b8a04f5126961522&quot;), &quot;destination&quot; : &quot;B&quot;, &quot;source&quot; : &quot;A&quot;, &quot;state&quot; : &quot;done&quot;, &quot;value&quot; : 100 &#125; 从失败场景中恢复：最重要的部分不是上面的典型例子，而是从各种失败场景中恢复未完成的事务的可能性。这部分将概述可能的失败，并提供方法从这些事件中恢复事务。这里有两种类型的失败： 1、所有发生在第一步（即设置事务的初始状态initial）之后，但在第三步（即应用事务到两个账户）之前的失败。为了还原事务，应用应该获取一个pending状态的transaction列表并且从第二步（即切换事务到pending状态）中恢复。 2、所有发生在第三步之后（即应用事务到两个账户）但在第五步(即设置事务状态为done)之前的失败。为了还原事务，应用需要获取一个committed状态的事务列表，并且从第四步（即移除pending事务）恢复。 因此应用程序总是能够恢复事务，最终达到一个一致的状态。应用程序开始捕获到每个未完成的事务时运行下面的恢复操作。你可能还希望定期运行恢复操作，以确保数据处于一致状态。达成一致状态所需要的时间取决于应用程序需要多长时间恢复每个事务。 回滚：在某些情况下可能需要“回滚”或“撤消”事务，当应用程序需要“取消”该事务时，或者是因为它永远需要恢复当其中一个帐户不存在的情况下，或停止现有的事务。这里有两种可能的回滚操作： 1、应用事务（即第三步）之后，你已经完全提交事务，你不应该回滚事务。相反，创建一个新的事务，切换源(源)和目标(destination)的值。 2、创建事务（即第一步）之后，在应用事务（即第三步）之前，使用下面的处理过程： 设置事务状态为canceling：首先设置事务状态为canceling，使用下面的update()操作： 1db.transactions.update(&#123;_id: t._id&#125;, &#123;$set: &#123;state: &quot;canceling&quot;&#125;&#125;) ###撤销事务： 使用下面的操作顺序从两个账户中撤销事务： 123db.accounts.update(&#123;name: t.source, pendingTransactions: t._id&#125;, &#123;$inc: &#123;balance: t.value&#125;, $pull: &#123;pendingTransactions: t._id&#125;&#125;)db.accounts.update(&#123;name: t.destination, pendingTransactions: t._id&#125;, &#123;$inc: &#123;balance: -t.value&#125;, $pull: &#123;pendingTransactions: t._id&#125;&#125;)db.accounts.find() find()操作返回acounts集合的内容，应该类似下面：12&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc97fb8a04f5126961523&quot;), &quot;balance&quot; : 1000, &quot;name&quot; : &quot;A&quot;, &quot;pendingTransactions&quot; : [ ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;4d7bc984b8a04f5126961524&quot;), &quot;balance&quot; : 1000, &quot;name&quot; : &quot;B&quot;, &quot;pendingTransactions&quot; : [ ] &#125; 设置事务状态为canceled：最后，使用下面的update()状态将事务状态设置为canceled：1db.transactions.update(&#123;_id: t._id&#125;, &#123;$set: &#123;state: &quot;canceled&quot;&#125;&#125;)","tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"http://jishusuishouji.github.io/tags/MongoDB/"}]},{"title":"mysql-Innodb事务隔离级别-repeatable read详解","date":"2017-03-27T11:04:08.000Z","path":"2017/03/27/mysql/mysql-Innodb事务隔离级别-repeatable_read详解/","text":"一、事务隔离级别ANSI/ISO SQL标准定义了4中事务隔离级别：未提交读（read uncommitted），提交读（read committed），重复读（repeatable read），串行读（serializable）。 对于不同的事务，采用不同的隔离级别分别有不同的结果。不同的隔离级别有不同的现象。主要有下面3种现在： 1、脏读（dirty read）：一个事务可以读取另一个尚未提交事务的修改数据。 2、非重复读（nonrepeatable read）：在同一个事务中，同一个查询在T1时间读取某一行，在T2时间重新读取这一行时候，这一行的数据已经发生修改(T1和T2都在同一个事务里面)，可能被更新了（update），也可能被删除了（delete）。 3、幻像读（phantom read）：在同一事务中，同一查询多次进行时候，由于其他插入操作（insert）的事务提交，导致每次返回不同的结果集。 不同的隔离级别有不同的现象，并有不同的锁定/并发机制，隔离级别越高，数据库的并发性就越差，4种事务隔离级别分别表现的现象如下表： 隔离级别 脏读 非重复读 幻像读 read uncommitted 允许 允许 允许 read committed 允许 允许 repeatable read 允许 serializable 二、数据库中的默认事务隔离级别在Oracle中默认的事务隔离级别是提交读（read committed）。对于MySQL的Innodb的默认事务隔离级别是重复读（repeatable read）。可以通过下面的命令查看： 12345678910111213mysql&gt; SELECT @@GLOBAL.tx_isolation, @@tx_isolation;+———————–+—————–+| @@GLOBAL.tx_isolation | @@tx_isolation |+———————–+—————–+| REPEATABLE-READ | REPEATABLE-READ |+———————–+—————–+1 row in set (0.00 sec) 下面进行一下测试： 【说明】事务提交，看到最新数据。 上面的结果可以看到Innodb的重复读（repeatable read）不允许脏读，不允许非重复读（即可以重复读，Innodb使用多版本一致性读来实现）和不允许幻象读（这点和ANSI/ISO SQL标准定义的有所区别）。 另外，同样的测试： 1、当session 2进行truncate表的时候，这个时候session 1再次查询就看不到数据。 2、当session 2进行alter表的时候，这个时候session 1再次查询就看不到数据。 造成以上的原因是因为 mysql的持续非锁定读，在repeatable read级别下，读采用的是持续非锁定读。相关介绍见下面： 持续读意味着InnoDB使用它的多版本化来给一个查询展示某个时间点处数据库的快照。查询看到在那个时间点之前被提交的那些确切事务做的更改，并且没有其后的事务或未提交事务做的改变。这个规则的例外是，查询看到发布该查询的事务本身所做的改变。 如果你运行在默认的REPEATABLE READ隔离级别，则在同一事务内的所有持续读读取由该事务中第一个这样的读所确立的快照。你可以通过提交当前事务并在发布新查询的事务之后，为你的查询获得一个更新鲜的快照。 持续读是默认模式，在其中InnoDBzai在READ COMMITTED和REPEATABLE READ隔离级别处理SELECT语句。持续读不在任何它访问的表上设置锁定，因此，其它用户可自由地在持续读在一个表上执行的同一时间修改这些表。 注意，持续读不在DROP TABLE和ALTER TABLE上作用。持续读不在DROP TABLE上作用，因为MySQL不能使用已经被移除的表，并且InnoDB 破坏了该表。持续读不在ALTER TABLE上作用，因为它在某事务内执行，该事务创建一个新表，并且从旧表往新表中插入行。现在，当你重新发出持续读之时，它不能在新表中看见任何行，因为它们被插入到一个在持续读读取的快照中不可见的事务 里。 MySQL官方文档中的多版本一致性读中说明了原因：Consistent read does not work over certain DDL statements。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://jishusuishouji.github.io/tags/mysql/"}]},{"title":"说说MySQL中的事务","date":"2017-03-27T10:27:04.000Z","path":"2017/03/27/mysql/说说MySQL中的事务/","text":"从一个问题开始从ATM机取钱分为以下几个步骤： 1.登陆ATM机，输入密码；2.连接数据库，验证密码；3.验证成功，获得用户信息，比如存款余额等；4.用户输入需要取款的金额，按下确认键；5.从后台数据库中减掉用户账户上的对应金额；6.ATM吐出钱；7.用户把钱拿走。 一个简单的取钱，主要分为以上几步。不知道大家有没有“天真”的想过，如果在第5步中，后台数据库中已经把钱减掉了，但是ATM还就是没有吐出钱（虽然实际也发生过，但是毕竟是低概率事件），这该怎么办？ 关于这个问题，银行系统的开发人员早就想过了，那么他们是怎么来搞定这个问题的呢？这就要说到今天总结的事务这个概念了。 简单说说事务对于上面的取钱这个事情，如果有一步出现了错误，那么就取消整个取钱的动作；简单来说，就是取钱这7步，要么都完成，要么就啥也不做。在数据库中，事务也是这个道理。 事务由一条或者多条sql语句组成，在事务中的操作，这些sql语句要么都执行，要么都不执行，这就是事务的目的。 对于事务而言，它需要满足ACID特性，下面就简要的说说事务的ACID特性。 A，表示原子性；原子性指整个数据库事务是不可分割的工作单位。只有使事务中所有的数据库操作都执行成功，整个事务的执行才算成功。事务中任何一个sql语句执行失败，那么已经执行成功的sql语句也必须撤销，数据库状态应该退回到执行事务前的状态；C，表示一致性；也就是说一致性指事务将数据库从一种状态转变为另一种一致的状态，在事务开始之前和事务结束以后，数据库的完整性约束没有被破坏；I，表示隔离性；隔离性也叫做并发控制、可串行化或者锁。事务的隔离性要求每个读写事务的对象与其它事务的操作对象能相互分离，即该事务提交前对其它事务都不可见，这通常使用锁来实现；D，持久性，表示事务一旦提交了，其结果就是永久性的，也就是数据就已经写入到数据库了，如果发生了宕机等事故，数据库也能将数据恢复。 总结了一些事务的基本概念，在MySQL中，事务还是分为很多中的，下面就来看看到底有哪些事务。 有哪些事务你能想象到吗？就这么个破事务还会分以下这么多种： 扁平事务； 带有保存点的扁平事务； 链事务； 嵌套事务； 分布式事务。 现在就来对这些事务从概念的层面上进行简单的总结一下。 扁平事务扁平事务是最简单的一种，也是实际开发中使用的最多的一种事务。在这种事务中，所有操作都处于同一层次，最常见的方式如下：1234567BEGIN WORK Operation 1 Operation 2 Operation 3 ... Operation NCOMMIT WORK 或者是这种：12345678BEGIN WORK Operation 1 Operation 2 Operation 3 ... Operation N (Error Occured)ROLLBACK WORK 扁平事务的主要缺点是不能提交或回滚事务的某一部分，或者分几个独立的步骤去提交。比如有这样的一个例子，我从呼和浩特去深圳，为了便宜，我可能这么干：1234BEGIN WORK Operation1:呼和浩特---火车---&gt;北京 Operation2:北京---飞机---&gt;深圳ROLLBACK WORK 但是，如果Operation1，从呼和浩特到北京的火车晚点了，错过了航班，怎么办？感觉扁平事务的特性，那我就需要回滚，我再回到呼和浩特，那么这样成本是不是也太高了啊，所以就有了下面的第二种事务——带有保存点的扁平事务。 带有保存点的扁平事务这种事务除了支持扁平事务支持的操作外，允许在事务执行过程中回滚到同一事务中较早的一个状态，这是因为可能某些事务在执行过程中出现的错误并不会对所有的操作都无效，放弃整个事务不合乎要求，开销也太大。保存点用来通知系统应该记住事务当前的状态，以便以后发生错误时，事务能回到该状态。 链事务链事务，就是指回滚时，只能恢复到最近一个保存点；而带有保存点的扁平事务则可以回滚到任意正确的保存点。 嵌套事务看下面这个，你就能明白了，啥是嵌套事务：123456789101112131415BEGIN WORK SubTransaction1: BEGIN WORK SubOperationX COMMIT WORK SubTransaction2: BEGIN WORK SubOperationY COMMIT WORK ... SubTransactionN: BEGIN WORK SubOperationN COMMIT WORKCOMMIT WORK 这就是嵌套事务，在事务中再嵌套事务，位于根节点的事务称为顶层事务。事务的前驱称为父事务，其它事务称为子事务。事务的前驱称为父事务，事务的下一层称为子事务。 子事务既可以提交也可以回滚，但是它的提交操作并不马上生效，除非由其父事务提交。因此就可以确定，任何子事务都在顶层事务提交后才真正的被提交了。同理，任意一个事务的回滚都会引起它的所有子事务一同回滚。 分布式事务分布式事务通常是指在一个分布式环境下运行的扁平事务，因此需要根据数据所在位置访问网络中的不同节点，比如：通过建设银行向招商银行转账，建设银行和招商银行肯定用的不是同一个数据库，同时二者的数据库也不在一个网络节点上，那么当用户跨行转账，就是通过分布式事务来保证数据的ACID的。 MySQL中使用事务理论总结的再好，终归都要通过实践来进行理解。下面就来说说MySQL中是如何使用事务的。 在MySQL命令行的默认设置下，事务都是自动提交的，即执行SQL语句后就会马上执行COMMIT操作。因此要显示地开启一个事务须使用命令BEGIN或START TRANSACTION，或者执行命令SET AUTOCOMMIT=0，用来禁止使用当前会话的自动提交。 来看看我们可以使用哪些事务控制语句。 BEGIN或START TRANSACTION；显示地开启一个事务； COMMIT；也可以使用COMMIT WORK，不过二者是等价的。COMMIT会提交事务，并使已对数据库进行的所有修改称为永久性的； ROLLBACK；有可以使用ROLLBACK WORK，不过二者是等价的。回滚会结束用户的事务，并撤销正在进行的所有未提交的修改； SAVEPOINT identifier；SAVEPOINT允许在事务中创建一个保存点，一个事务中可以有多个SAVEPOINT； RELEASE SAVEPOINT identifier；删除一个事务的保存点，当没有指定的保存点时，执行该语句会抛出一个异常； ROLLBACK TO identifier；把事务回滚到标记点； SET RANSACTION；用来设置事务的隔离级别。InnoDB存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ和SERIALIZABLE。 这些不用你“管”有的时候有些SQL语句会产生一个隐式的提交操作，即执行完成这些语句后，会有一个隐式的COMMIT操作。有以下SQL语句，不用你去“管”： DDL语句，ALTER DATABASE、ALTER EVENT、ALTER PROCEDURE、ALTER TABLE、ALTER VIEW、CREATE TABLE、DROP TABLE、RENAME TABLE、TRUNCATE TABLE等； 修改MYSQL架构的语句，CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD； 管理语句，ANALYZE TABLE、CACHE INDEX、CHECK TABLE、LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE等。 以上的这些SQL操作都是隐式的提交操作，不需要手动显式提交。 事务的隔离级别上面也说到了SET TRANSACTION用来设置事务的隔离级别。那事务的隔离级别是什么东东？ 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。 InnoDB存储引擎提供事务的隔离级别有READ UNCOMMITTED、READ COMMITTED、REPEATABLE READ和SERIALIZABLE。这些隔离级别之间的区别如下： 隔离级别 脏读（Dirty Read） 不可重复读（NonRepeatable Read） 幻读（Phantom Read） 未提交读（Read uncommitted） 可能 可能 可能 已提交读（Read committed） 不可能 可能 可能 可重复读（Repeatable read） 不可能 不可能 可能 可串行化（Serializable ） 不可能 不可能 不可能 脏读：一个事务读取到了另外一个事务没有提交的数据；比如：事务T1更新了一行记录的内容，但是并没有提交所做的修改。事务T2读取到了T1更新后的行，然后T1执行回滚操作，取消了刚才所做的修改。现在T2所读取的行就无效了；不可重复读：在同一事务中，两次读取同一数据，得到内容不同；比如：事务T1读取一行记录，紧接着事务T2修改了T1刚才读取的那一行记录(T2的事务已经提交了)。然后T1又再次读取这行记录，发现与刚才读取的结果不同。这就称为“不可重复”读，因为T1原来读取的那行记录已经发生了变化；幻读：同一事务中，用同样的操作读取两次，得到的记录数不相同；比如：事务T1读取一条指定的WHERE子句所返回的结果集。然后事务T2新插入一行记录，这行记录恰好可以满足T1所使用的查询条件中的WHERE子句的条件。然后T1又使用相同的查询再次对表进行检索，但是此时却看到了事务T2刚才插入的新行。这个新行就称为“幻像”，因为对T1来说这一行就像突然出现的一样。 隔离级别越低，事务请求的锁越少或保持锁的时间就越短。InnoDB存储引擎默认的支持隔离级别是REPEATABLE READ；在这种默认的事务隔离级别下已经能完全保证事务的隔离性要求，即达到SQL标准的SERIALIZABLE级别隔离。 我们可以可以用SET TRANSACTION语句改变单个会话或者所有新进连接的隔离级别。它的语法如下：1SET [SESSION | GLOBAL] TRANSACTION ISOLATION LEVEL &#123;READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE&#125; 注意：默认的行为（不带session和global）是为下一个（未开始）事务设置隔离级别。如果使用GLOBAL关键字，语句在全局对从那点开始创建的所有新连接（除了不存在的连接）设置默认事务级别。你需要SUPER权限来做这个。使用SESSION 关键字为将来在当前连接上执行的事务设置默认事务级别。 任何客户端都能自由改变会话隔离级别（甚至在事务的中间），或者为下一个事务设置隔离级别。12345678910mysql&gt; set session transaction isolation level repeatable read;Query OK, 0 rows affected (0.00 sec)mysql&gt; select @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+1 row in set (0.00 sec) 数据库的默认隔离级别mysql的默认隔离级别是可重复读：要是读为主的业务场景，建议RC模式；若是非读为主的业务场景，则建议RR模式，考虑到MySQL5.1及以上版本二进制日志登记格式，建议优先考虑RR模式。 Oracle采用的也是 read committedOracle的RC 跟InnoDB存储引擎的RC不是一样的，属于综合了 RC + RR的折中版本。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://jishusuishouji.github.io/tags/mysql/"}]},{"title":"ORM到底是用还是不用？","date":"2017-03-27T05:22:54.000Z","path":"2017/03/27/ORM/ORM到底是用还是不用？/","text":"ORM即Object/Relation Mapping的简写，一般称作“对象关系映射”，在Web开发中最常出没于和关系型数据库交互的地方。接口、中间件、库、包，你都可以这么称呼它。我们可以结合PHP和MySQL，从ORM的四个核心理念来认识它： 简单：ORM以最基本的形式建模数据。比如ORM会将MySQL的一张表映射成一个PHP类（模型），表的字段就是这个类的成员变量 精确：ORM使所有的MySQL数据表都按照统一的标准精确地映射成PHP类，使系统在代码层面保持准确统一 易懂：ORM使数据库结构文档化。比如MySQL数据库就被ORM转换为了PHP程序员可以读懂的PHP类，PHP程序员可以只把注意力放在他擅长的PHP层面（当然能够熟练掌握MySQL更好） 易用：ORM的避免了不规范、冗余、风格不统一的SQL语句，可以避免很多人为Bug，方便编码风格的统一和后期维护 接下来再通过一个很基本的例子来说明一下ORM的使用，还以PHP和MySQL为例。 user这个数据模型是再普遍不过的了。假设我们有一张user数据表。 在OOP中通常我们需要写一个对应的class User来作为user数据表的数据模型:12345678// 声明class Userclass User&#123; $id; $name; function create()&#123;/*...*/&#125; function load($id)&#123;/*...*/&#125;&#125; 1234// 使用class User$user = new User();$user-&gt;name = &apos;fancy&apos;;$user-&gt;create(); 但是通过ORM，我们可以不用去声明class User，可以直接继承ORM提供的工厂类，比如：1234// 直接使用！对于熟悉MVC的亲知道这个意义之所在！$user = new ORM(&apos;user&apos;); // ORM都有自己的规则，这里直接使用了MySQL的表名$user-&gt;name = &apos;fancy&apos;; // MySQL的表的字段就是$user对象的成员变量$user-&gt;save(); // 掉用ORM提供的接口函数 ORM一般都针对数据模型提供了一下常见的接口函数，比如：create(), update(), save(), load(), find(), find_all(), where()等，也就是讲sql查询全部封装成了编程语言中的函数，通过函数的链式组合生成最终的SQL语句。 所以由这些来看，ORM对于敏捷开发和团队合作开发来说，好处是非常非常大的。这里就罗列一下我想到的ORM显著的优点： 大大缩短了程序员的编码时间，减少甚至免除了对Model的编码 良好的数据库操作接口，使编码难度降低，使团队成员的代码变得简洁易读、风格统一 动态的数据表映射，在数据表结构甚至数据库发生改变时，减少了相应的代码修改 减少了程序员对数据库的学习成本 可以很方便地引入数据缓存之类的附加功能 但是ORM并不是一个完美的东西，它同时也有其自身不可避免的缺点： 自动化进行关系数据库的映射需要消耗系统性能。其实这里的性能消耗还好啦，一般来说都可以忽略之，特别是有cacha存在的时候 在处理多表联查、where条件复杂之类的查询时，ORM的语法会变得复杂且猥琐 越是功能强大的ORM越是消耗内存，因为一个ORM Object会带有很多成员变量和成员函数。有一次修复bug时就遇见，使用ORM查询的时候会占用12MB的内存，而使用SQL的查询时只占用了1.7MB…… ORM就是这么一个让人又爱又恨的东西。回到我们开始的问题：“ORM到底是用还是不用？” Fancy个人的观点是：ORM要用！但关键部位不能用！因为对于一般的Web应用开发来说，使用ORM确实能带来上述的诸多好处，而且在大部分情况下涉及不到ORM的不好的地方。但是在系统里面有大数据量、大运算量、复杂查询的地方，就不要用ORM。ORM的性能问题将给你带来灾难。在这些地方就可以使用纯SQL或者其他简单轻量的DB Helper库了。在详细了解ORM之后，你就可以扬长避短让ORM发挥其最大效用了。","tags":[{"name":"ORM","slug":"ORM","permalink":"http://jishusuishouji.github.io/tags/ORM/"}]},{"title":"maven 多模块项目","date":"2017-03-27T02:16:27.000Z","path":"2017/03/27/maven/maven_多模块项目/","text":"","tags":[]},{"title":"Maven最佳实践：划分模块","date":"2017-03-27T02:00:50.000Z","path":"2017/03/27/maven/Maven最佳实践：划分模块/","text":"“分天下为三十六郡，郡置守，尉，监” —— 《史记·秦始皇本纪》 所有用Maven管理的真实的项目都应该是分模块的，每个模块都对应着一个pom.xml。它们之间通过继承和聚合（也称作多模块，multi-module）相互关联。那么，为什么要这么做呢？我们明明在开发一个项目，划分模块后，导入Eclipse变成了N个项目，这会带来复杂度，给开发带来不便。为了解释原因，假设有这样一个项目，很常见的Java Web应用。在这个应用中，我们分了几层： Dao层负责数据库交互，封装了Hibernate交互的类。 Service层处理业务逻辑，放一些Service接口和实现相关的Bean。 Web层负责与客户端交互，主要有一些Structs的Action类。 对应的，在一个项目中，我们会看到一些包名： org.myorg.app.dao org.myorg.app.service org.myorg.app.web org.myorg.app.util 这样整个项目的框架就清晰了，但随着项目的进行，你可能会遇到如下问题：这个应用可能需要有一个前台和一个后台管理端（web或者swing），你发现大部分dao，一些service，和大部分util是在两个应用中可用的。这样的问题，你一周内遇到了好几次。pom.xml中的依赖列表越来越长以重用的，但是，由于目前只有一个项目（WAR），你不得不新建一个项目依赖这个WAR，这变得非常的恶心，因为在Maven中配置对WAR的依赖远不如依赖JAR那样简单明了，而且你根本不需要org.myorg.app.web。有人修改了dao，提交到svn并且不小心导致build失败了，你在编写service的代码，发现编译不过，只能等那人把dao修复了，你才能继续进行，很多人都在修改，到后来你根本就不清楚哪个依赖是谁需要的，渐渐的，很多不必要的依赖被引入。甚至出现了一个依赖有多个版本存在。build整个项目的时间越来越长，尽管你只是一直在web层工作，但你不得不build整个项目。某个模块，比如util，你只想让一些经验丰富的人来维护，可是，现在这种情况，每个开发者都能修改，这导致关键模块的代码质量不能达到你的要求。我们会发现，其实这里实际上没有遵守一个设计模式原则：“高内聚，低耦合”。虽然我们通过包名划分了层次，并且你还会说，这些包的依赖都是单向的，没有包的环依赖。这很好，但还不够，因为就构建层次来说，所有东西都被耦合在一起了。因此我们需要使用Maven划分模块。 一个简单的Maven模块结构是这样的：1234567891011121314---- app-parent |-- pom.xml (pom) | |-- app-util | |-- pom.xml (jar) | |-- app-dao | |-- pom.xml (jar) | |-- app-service | |-- pom.xml (jar) | |-- app-web |-- pom.xml (war) 上述简单示意图中，有一个父项目(app-parent)聚合很多子项目（app-util, app-dao, app-service, app-web）。每个项目，不管是父子，都含有一个pom.xml文件。而且要注意的是，小括号中标出了每个项目的打包类型。父项目是pom,也只能是pom。子项目有jar，或者war。根据它包含的内容具体考虑。 这些模块的依赖关系如下：123app-dao --&gt; app-utilapp-service --&gt; app-daoapp-web --&gt; app-service 注意依赖的传递性（大部分情况是传递的，除非你配置了特殊的依赖scope），app-dao依赖于app-util，app-service依赖于app-dao，于是app-service也依赖于app-util。同理，app-web依赖于app-dao,app-util。 用项目层次的划分替代包层次的划分能给我们带来如下好处： 方便重用，如果你有一个新的swing项目需要用到app-dao和app-service，添加对它们的依赖即可，你不再需要去依赖一个WAR。而有些模块，如app-util，完全可以渐渐进化成公司的一份基础工具类库，供所有项目使用。这是模块化最重要的一个目的。 由于你现在划分了模块，每个模块的配置都在各自的pom.xml里，不用再到一个混乱的纷繁复杂的总的POM中寻找自己的配置。 如果你只是在app-dao上工作，你不再需要build整个项目，只要在app-dao目录运行mvn命令进行build即可，这样可以节省时间，尤其是当项目越来越复杂，build越来越耗时后。 某些模块，如app-util被所有人依赖，但你不想给所有人修改，现在你完全可以从这个项目结构出来，做成另外一个项目，svn只给特定的人访问，但仍提供jar给别人使用。 多模块的Maven项目结构支持一些Maven的更有趣的特性（如DepencencyManagement），这留作以后讨论。 接下来讨论一下POM配置细节，实际上非常简单，先看app-parent的pom.xml：1234567891011121314&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.myorg.myapp&lt;/groupId&gt; &lt;artifactId&gt;app-parent&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;modules&gt; &lt;module&gt;app-util&lt;/module&gt; &lt;module&gt;app-dao&lt;/module&gt; &lt;module&gt;app-service&lt;/module&gt; &lt;module&gt;app-web&lt;/module&gt; &lt;/modules&gt; &lt;/project&gt; Maven的坐标GAV（groupId, artifactId, version）在这里进行配置，这些都是必须的。特殊的地方在于，这里的packaging为pom。所有带有子模块的项目的packaging都为pom。packaging如果不进行配置，它的默认值是jar，代表Maven会将项目打成一个jar包。该配置重要的地方在于modules，例子中包含的子模块有app-util, app-dao, app-service, app-war。在Maven build app-parent的时候，它会根据子模块的相互依赖关系整理一个build顺序，然后依次build。这就是一个父模块大概需要的配置，接下来看一下子模块符合配置继承父模块。1234567891011121314151617&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;app-parent&lt;/artifactId&gt; &lt;groupId&gt;org.myorg.myapp&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;app-util&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;commons-lang&lt;/groupId&gt; &lt;artifactId&gt;commons-lang&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; app-util模块继承了app-parent父模块，因此这个POM的一开始就声明了对app-parent的引用，该引用是通过Maven坐标GAV实现的。而关于项目app-util本身，它却没有声明完整GAV，这里我们只看到了artifactId。这个POM并没有错，groupId和version默认从父模块继承了。实际上子模块从父模块继承一切东西，包括依赖，插件配置等等。此外app-util配置了一个对于commons-lang的简单依赖，这是最简单的依赖配置形式。大部分情况，也是通过GAV引用的。再看一下app-dao，它也是继承于app-parent，同时依赖于app-util：1234567891011121314151617&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;app-parent&lt;/artifactId&gt; &lt;groupId&gt;org.myorg.myapp&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;app-dao&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.myorg.myapp&lt;/groupId&gt; &lt;artifactId&gt;app-util&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; 该配置和app-util的配置几乎没什么差别，不同的地方在于，依赖变化了，app-dao依赖于app-util。这里要注意的是version的值为${project.version}，这个值是一个属性引用，指向了POM的project/version的值，也就是这个POM对应的version。由于app-dao的version继承于app-parent，因此它的值就是1.0-SNAPSHOT。而app-util也继承了这个值，因此在所有这些项目中，我们做到了保持版本一致。这里还需要注意的是，app-dao依赖于app-util，而app-util又依赖于commons-lang，根据传递性，app-dao也拥有了对于commons-lang的依赖。app-service我们跳过不谈，它依赖于app-dao。我们最后看一下app-web：123456789101112131415161718&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;app-parent&lt;/artifactId&gt; &lt;groupId&gt;org.myorg.myapp&lt;/groupId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;app-web&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.myorg.myapp&lt;/groupId&gt; &lt;artifactId&gt;app-service&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; app-web依赖于app-service，因此配置了对其的依赖。由于app-web是我们最终要部署的应用，因此它的packaging是war。为此，你需要有一个目录src/main/webapp。并在这个目录下拥有web应用需要的文件，如/WEB-INF/web.xml。没有web.xml，Maven会报告build失败，此外你可能还会有这样一些子目录：/js, /img, /css … 。 看看Maven是如何build整个项目的，我们在 app-parent 根目录中运行 mvn clean install ，输出的末尾会有大致这样的内容：12345678910111213141516171819202122232425262728......[INFO] [war:war][INFO] Packaging webapp[INFO] Assembling webapp[app-web] in [/home/juven/workspaces/ws-others/myapp/app-web/target/app-web-1.0-SNAPSHOT][INFO] Processing war project[INFO] Webapp assembled in[50 msecs][INFO] Building war: /home/juven/workspaces/ws-others/myapp/app-web/target/app-web-1.0-SNAPSHOT.war[INFO] [install:install][INFO] Installing /home/juven/workspaces/ws-others/myapp/app-web/target/app-web-1.0-SNAPSHOT.war to /home/juven/.m2/repository/org/myorg/myapp/app-web/1.0-SNAPSHOT/app-web-1.0-SNAPSHOT.war[INFO] [INFO] [INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] ------------------------------------------------------------------------[INFO] app-parent ............................................ SUCCESS [1.191s][INFO] app-util .............................................. SUCCESS [1.274s][INFO] app-dao ............................................... SUCCESS [0.583s][INFO] app-service ........................................... SUCCESS [0.593s][INFO] app-web ............................................... SUCCESS [0.976s][INFO] ------------------------------------------------------------------------[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESSFUL[INFO] ------------------------------------------------------------------------[INFO] Total time: 4 seconds[INFO] Finished at: Sat Dec 27 08:20:18 PST 2008[INFO] Final Memory: 3M/17M[INFO] ------------------------------------------------------------------------ 注意Reactor Summary，整个项目根据我们希望的顺序进行build。Maven根据我们的依赖配置，智能的安排了顺序，app-util, app-dao, app-service, app-web。 最后，你可以在 app-web/target 目录下找到文件 app-web-1.0-SNAPSHOT.war ，打开这个war包，在 /WEB-INF/lib 目录看到了 commons-lang-2.4.jar，以及对应的app-util, app-dao, app-service 的jar包。Maven自动帮你处理了打包的事情，并且根据你的依赖配置帮你引入了相应的jar文件。 使用多模块的Maven配置，可以帮助项目划分模块，鼓励重用，防止POM变得过于庞大，方便某个模块的构建，而不用每次都构建整个项目，并且使得针对某个模块的特殊控制更为方便。本文同时给出了一个实际的配置样例，展示了如何使用Maven配置多模块项目。","tags":[{"name":"maven","slug":"maven","permalink":"http://jishusuishouji.github.io/tags/maven/"}]},{"title":"mongodb最大连接数修改","date":"2017-03-26T11:22:25.000Z","path":"2017/03/26/mongodb/mongodb最大连接数修改/","text":"在nodejs启动时一次性开了200个Mongodb连接，目的是为了高并发时减少数据库连接耗时。如果做cluster开10个实例就有2000个连接了，这样就有些节点连接不到数据库的情况。 原因是Mongodb默认最大连接数只有819个，于是通过在启动里面加参数--maxConns=3000来提高最大连接数。然后重启服务，但悲剧的是通过db.serverStatus().connections;查看到最大连接数还是819。原因是linux系统的限制，Linux系统默认一个进程最大文件打开数目为1024。需要在Mongodb开启前修改这个限制。在运行数据前运行ulimit -n命令 。如果已经加入开机脚本，就要在脚本中启动前增加这行了。比如： ulimit -n 20000 /usr/mongodb/bin/mongod --dbpath=/usr/mongodb/data/ --logpath=/usr/mongodb/log/mongodb.log --maxConns=3000 --fork 再查看就可以看到最大连接数增加了。 重启机器后仍有问题解决问题：Invariant failure: ret resulted in status UnknownError 24: Too many open files at src/mongo/db/storage/wiredtiger/wiredtiger_session_cache.cpp 73 按照官方的建议https://docs.mongodb.com/manual/reference/ulimit/#recommended-ulimit-settings， 由于centos 6的最大进程连接数为1024，我们就增加一个限制设定的配置 Red Hat Enterprise Linux and CentOS 6 place a max process limitation of 1024 which overridesulimit settings. Create a file named /etc/security/limits.d/99-mongodb-nproc.conf with new soft nproc and hard nproc values to increase the process limit. See /etc/security/limits.d/90-nproc.conf file as an example. 按照官方推荐的设置123456-f (file size): unlimited-t (cpu time): unlimited-v (virtual memory): unlimited [1]-n (open files): 64000-m (memory size): unlimited [1] [2]-u (processes/threads): 64000 由于服务器只有openfiles不匹配且比推荐的小，另外process/threads比较大， 所以其中99-mongodb-nproc.conf的内容如下：12345678# Default limit for number of user&apos;s processes to prevent# accidental fork bombs.# See rhbz #432903 for reasoning. root soft nproc unlimitedroot hard nproc unlimitedroot soft nofile 64000root hard nofile 64000 设计后重启机器，可用ulimit -a看到值已经更改，问题解决。","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://jishusuishouji.github.io/tags/mongodb/"}]},{"title":"Spring Cloud Netflix构建微服务入门实践","date":"2017-03-26T09:57:46.000Z","path":"2017/03/26/spring/Spring_Cloud_Netflix构建微服务入门实践/","text":"在使用Spring Cloud Netflix构建微服务之前，我们先了解一下Spring Cloud集成的Netflix OSS的基础组件Eureka，对于Netflix的其他微服务组件，像Hystrix、Zuul、Ribbon等等本文暂不涉及，感兴趣可以参考官网文档。这里，我们用最基础的Eureka来构建一个最基础的微服务应用，来演示如何构建微服务，了解微服务的基本特点。 EurekaEureka是Netflix开源的一个微服务注册组件，提供服务发现特性，它是一个基于REST的服务，主要具有如下功能： 支持服务注册和发现 具有Load Balance和Failover的功能 在进行服务调用过程中，无需知道目标服务的主机（IP）和端口，只要知道服务名就可以实现调用 通过Netfix在Github上的文档，我们看一下Eureka的基本架构，如下图所示：Eureka主要包含如下两个核心组件： Eureka ServerEureka Server是服务注册的服务端组件，负责管理Eureka Client注册的服务，提供服务发现的功能。它支持集群模式部署，集群部署模式中，多个Eureka Server之间会同步服务注册数据，能够保证某一个Eureka Server因为故障挂掉，仍能对外提供注册服务的能力。因为最初在Netflix，Eureka主要用在AWS Cloud上，用作定位服务、Load Balance和Failover，在AWS Cloud上，Eureka支持在多个Region中部署Eureka Server而构建一个注册中心集群，从而实现了服务注册中心的高可用性。 Eureka ClientEureka Client是Eureka Server客户端组件库，可以基于它向Eureka Server注册服务，供服务调用方调用；也可以是一个服务调用方，通过检索服务并调用已经注册的服务。如上图所示，Application Service和Application Client都是基于Eureka Client开发的使用Eureka Server的服务。另外，Eureka Client提供了内置的Load Balancer，实现了基本的Round-robin模式的负载均衡。 Spring Cloud NetflixSpring Cloud Netflix提供了对Netflix OSS的集成，同时还使用了Spring Boot，能够极大地简化微服务程序的开发。使用Spring Cloud提供的基本注解，就能非常方便的使用Netfix OSS基本组件。要想使用Spring Cloud Eureka，只需要在Maven POM文件中加入如下依赖管理配置即可：1234567891011&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-netflix&lt;/artifactId&gt; &lt;version&gt;1.0.7.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 关于如何使用注解，我们会在下面的实践中，详细说明。 构建微服务实践我们构建一个简单的微服务应用，能够实现服务注册，服务调用的基本功能。计划实现的微服务应用，交互流程如下图所示：上图中，我们假设Eureka Client并没有缓存Eureka Server中注册的服务，而是每次都需要通过Eureka Server来查找并映射目标服务。上图所示的微服务应用，具有如下服务组件： 两个Eureka Server实例组成的服务发现集群通过Spring Cloud实现，只需要使用注解配置即可，代码如下所示： 01package org.shirdrn.springcloud.eureka.server;02 03import org.springframework.boot.autoconfigure.SpringBootApplication;04import org.springframework.boot.builder.SpringApplicationBuilder;05import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;06 07@EnableEurekaServer08@SpringBootApplication09public class MyEurekaServer {10 11 public static void main(String[] args) {12 new SpringApplicationBuilder(MyEurekaServer.class).web(true).run(args);13 }14}部署两个Eureka Server的代码是相同的，其中，对应的配置文件application.yml内容不同，示例如下所示： 01server:02 port: 330003spring:04 application:05 name: my-eureka-server06eureka:07 client:08 serviceUrl:09 defaultZone: http://localhost:3300/eureka/,http://localhost:3301/eureka/10 instance:11 metadataMap:12 instanceId: ${spring.application.name}:${spring.application.instance_id:${random.value}}另一个只需要改一下server.port为3301即可。 具有两个实例的Greeting Service服务该示例服务，只是提供一个接口，能够给调用方返回调用结果，实现代码，如下所示： 01package org.shirdrn.springcloud.eureka.applicationservice.greeting;02 03import org.springframework.boot.autoconfigure.EnableAutoConfiguration;04import org.springframework.boot.autoconfigure.SpringBootApplication;05import org.springframework.boot.builder.SpringApplicationBuilder;06import org.springframework.cloud.netflix.eureka.EnableEurekaClient;07import org.springframework.web.bind.annotation.PathVariable;08import org.springframework.web.bind.annotation.RequestMapping;09import org.springframework.web.bind.annotation.RequestMethod;10import org.springframework.web.bind.annotation.RestController;11 12@SpringBootApplication13@EnableEurekaClient14@RestController15@EnableAutoConfiguration16public class GreeingService {17 18 @RequestMapping(method = RequestMethod.GET, value = “/greeting/{name}”)19 public String greet(@PathVariable(“name”) String name) {20 return “::01:: Hello, “ + name + “!”;21 }22 23 public static void main(String[] args) {24 new SpringApplicationBuilder(GreeingService.class).web(true).run(args);25 }26}为了能够观察，Greeting Service的两个实例，能够在调用的时候实现Round-robin风格的负载均衡，特别在返回的结果中增加了标识来区分。对应的配置文件application.properties内容，除了对应的端口和服务实例名称不同，其它都相同，示例如下所示： 1server.port=99012spring.application.name = greeting.service3eureka.instance.metadataMap.instanceId = ${spring.application.name}:instance-99014eureka.client.serviceUrl.defaultZone = http://localhost:3300/eureka/,http://localhost:3301/eureka/这样就可以在启动时注册到Eureka Server中。 一个名称为Application Caller的服务，需要调用Greeting Service服务该服务和上面的服务类似，只是在其内部实现了对远程服务的调用，我们的实现代码如下所示： 01package org.shirdrn.springcloud.eureka.applicationclient.caller;02 03import org.springframework.beans.factory.annotation.Autowired;04import org.springframework.boot.CommandLineRunner;05import org.springframework.boot.autoconfigure.SpringBootApplication;06import org.springframework.boot.builder.SpringApplicationBuilder;07import org.springframework.cloud.netflix.eureka.EnableEurekaClient;08import org.springframework.cloud.netflix.feign.EnableFeignClients;09import org.springframework.stereotype.Component;10import org.springframework.web.client.RestTemplate;11 12@SpringBootApplication13@EnableEurekaClient14@EnableFeignClients15public class Application {16 17 public static void main(String[] args) {18 new SpringApplicationBuilder(Application.class)19 .web(false)20 .run(args);21 }22}23 24@Component25class RestTemplateExample implements CommandLineRunner {26 27 @Autowired28 private RestTemplate restTemplate;29 private static final String GREETING_SERVICE_URI = “http://greeting.service/greeting/{name}“; // 通过服务名称来调用，而不需要知道目标服务的IP和端口30 31 @Override32 public void run(String… strings) throws Exception {33 while(true) {34 String greetingSentence = this.restTemplate.getForObject(35 GREETING_SERVICE_URI,36 String.class,37 “Dean Shi”); // 透明调用远程服务38 System.out.println(“Response result: “ + greetingSentence);39 40 Thread.sleep(5000);41 }42 }43}对应的配置文件application.properties内容，如下所示： 1server.port=99992spring.application.name = application.client.caller3eureka.instance.metadataMap.instanceId = ${spring.application.name}:instance-99994eureka.client.serviceUrl.defaultZone = http://localhost:3300/eureka/,http://localhost:3301/eureka/启动并验证微服务应用 上面已经实现了该示例微服务应用的全部组件，先可以启动各个服务组件了。启动顺序如下所示： 启动两个Eureka Server启动两个Greeting Service启动服务消费应用Application Call可以通过Web页面查看Eureka Server控制台，如下图所示：eureka-web-console多次启动Application Call应用，就可以通过查看Greeting Service服务的日志，可以看到服务被调用，而且实现了基础的Round-robin负载均衡，日志如下所示： 1Response result: ::02:: Hello, Dean Shi!2Response result: ::01:: Hello, Dean Shi!3Response result: ::02:: Hello, Dean Shi!4Response result: ::01:: Hello, Dean Shi!5Response result: ::02:: Hello, Dean Shi!6Response result: ::01:: Hello, Dean Shi!我们实现示例微服务应用，验证后符合我们的期望。上面微服务应用的实现代码及其配置，可以查看我的Github：https://github.com/shirdrn/springcloud-eureka-demo.git 参考链接 https://github.com/Netflix/eureka/wiki/Eureka-at-a-glancehttp://cloud.spring.io/spring-cloud-netflix/http://cloud.spring.io/spring-cloud-netflix/1.0.x/https://spring.io/blog/2015/01/20/microservice-registration-and-discovery-with-spring-cloud-and-netflix-s-eurekahttp://itmuch.com/spring-cloud-sum-eureka/http://blog.abhijitsarkar.org/technical/netflix-eureka/Creative Commons License本文基于署名-非商业性使用-相同方式共享 4.0许可协议发布，欢迎转载、使用、重新发布，但务必保留文章署名时延军（包含链接：http://shiyanjun.cn），不得用于商业目的，基于本文修改后的作品务必以相同的许可发布。如有任何疑问，请与我联系。","tags":[{"name":"spring","slug":"spring","permalink":"http://jishusuishouji.github.io/tags/spring/"}]},{"title":"ZooKeeper 基础知识、部署和应用程序","date":"2017-03-25T16:07:53.000Z","path":"2017/03/26/zookeeper/ZooKeeper_基础知识、部署和应用程序/","text":"Apache ZooKeeper 是一个面向分布式应用程序的高性能协调服务器。它使用一个简单的接口暴露公共服务（比如命名和配置管理、同步和组服务），让用户不必从头开始编程。它为实现共识、组管理、领导者选举和到场协议（presence protocol）配备了现成的支持。 的示例。 简介ZooKeeper 是一个面向分布式系统的构建块。当设计一个分布式系统时，一般需要设计和开发一些协调服务： 名称服务— 名称服务是将一个名称映射到与该名称有关联的一些信息的服务。电话目录是将人的名字映射到其电话号码的一个名称服务。同样，DNS服务也是一个名称服务，它将一个域名映射到一个 IP 地址。在分布式系统中，您可能想跟踪哪些服务器或服务在运行，并通过名称查看其状态。ZooKeeper暴露了一个简单的接口来完成此工作。也可以将名称服务扩展到组成员服务，这样就可以获得与正在查找其名称的实体有关联的组的信息。 锁定— 为了允许在分布式系统中对共享资源进行有序的访问，可能需要实现分布式互斥（distributed mutexes）。ZooKeeper 提供一种简单的方式来实现它们。 同步— 与互斥同时出现的是同步访问共享资源的需求。无论是实现一个生产者-消费者队列，还是实现一个障碍，ZooKeeper 都提供一个简单的接口来实现该操作。 配置管理— 您可以使用 ZooKeeper 集中存储和管理分布式系统的配置。这意味着，所有新加入的节点都将在加入系统后就可以立即使用来自ZooKeeper的最新集中式配置。这还允许您通过其中一个 ZooKeeper 客户端更改集中式配置，集中地更改分布式系统的状态。 领导者选举— 分布式系统可能必须处理节点停机的问题，您可能想实现一个自动故障转移策略。ZooKeeper 通过领导者选举对此提供现成的支持。 虽然可以从头开始设计和实现所有这些服务，但调试任何问题、竞争条件或死锁都需要执行额外的工作，并且很难实现。就像您不会在代码中随处编写自己的随机数发生器或哈希函数一样，这里有一个要求：人们不应该在每次有需要时就到处从头编写自己的名称服务或领导者选举服务。此外，您可以相对容易地一起解决一个非常简单的组成员服务，但是，要编写它们来提供可靠性、复制和可扩展性，可能需要做更多的工作。这导致了Apache ZooKeeper 的开发和开源，Apache ZooKeeper是一个针对分布式系统的、开箱即用的、可靠的、可扩展的、高性能的协调服务。 ZooKeeper虽然是一个针对分布式系统的协调服务，但它本身也是一个分布式应用程序。ZooKeeper 遵循一个简单的客户端-服务器模型，其中客户端是使用服务的节点（即机器），而服务器是提供服务的节点。ZooKeeper 服务器的集合形成了一个ZooKeeper集合体（ensemble）。在任何给定的时间内，一个 ZooKeeper 客户端可连接到一个 ZooKeeper 服务器。每个 ZooKeeper服务器都可以同时处理大量客户端连接。每个客户端定期发送 ping 到它所连接的 ZooKeeper 服务器，让服务器知道它处于活动和连接状态。被询问的ZooKeeper 服务器通过 ping 确认进行响应，表示服务器也处于活动状态。如果客户端在指定时间内没有收到服务器的确认，那么客户端会连接到集合体中的另一台服务器，而且客户端会话会被透明地转移到新的 ZooKeeper 服务器。 ZooKeeper 有一个类似于文件系统的数据模型，由znodes组成。可以将 znodes（ZooKeeper 数据节点）视为类似 UNIX 的传统系统中的文件，但它们可以有子节点。另一种方式是将它们视为目录，它们可以有与其相关的数据。每个这些目录都被称为一个 znode。图 2 显示的图代表与两个城市中的运动队相同的层次结构。 znode层次结构被存储在每个 ZooKeeper服务器的内存中。这实现了对来自客户端的读取操作的可扩展的快速响应。每个 ZooKeeper服务器还在磁盘上维护了一个事务日志，记录所有的写入请求。因为ZooKeeper 服务器在返回一个成功的响应之前必须将事务同步到磁盘，所以事务日志也是ZooKeeper 中对性能最重要的组成部分。可以存储在 znode 中的数据的默认最大大小为 1 MB。因此，即使 ZooKeeper 的层次结构看起来与文件系统相似，也不应该将它用作一个通用的文件系统。相反，应该只将它用作少量数据的存储机制，以便为分布式应用程序提供可靠性、可用性和协调。当客户端请求读取特定 znode 的内容时，读取操作是在客户端所连接的服务器上进行的。因此，由于只涉及集合体中的一个服务器，所以读取是快速和可扩展的。然而，为了成功完成写入操作，要求 ZooKeeper 集合体的严格意义上的多数节点都是可用的。在启动 ZooKeeper 服务时，集合体中的某个节点被选举为领导者。当客户端发出一个写入请求时，所连接的服务器会将请求传递给领导者。此领导者对集合体的所有节点发出相同的写入请求。如果严格意义上的多数节点（也被称为法定数量（quorum））成功响应该写入请求，那么写入请求被视为已成功完成。然后，一个成功的返回代码会返回给发起写入请求的客户端。如果集合体中的可用节点数量未达到法定数量，那么ZooKeeper服务将不起作用。 法定数量是通过严格意义上的多数节点来表示的。在集合体中，可以包含一个节点，但它不是一个高可用和可靠的系统。如果在集合体中有两个节点，那么这两个节点都必须已经启动并让服务正常运行，因为两个节点中的一个并不是严格意义上的多数。如果在集合体中有三个节点，即使其中一个停机了，您仍然可以获得正常运行的服务（三个中的两个是严格意义上的多数）。出于这个原因，ZooKeeper 的集合体中通常包含奇数数量的节点，因为就容错而言，与三个节点相比，四个节点并不占优势，因为只要有两个节点停机，ZooKeeper 服务就会停止。在有五个节点的集群上，需要三个节点停机才会导致 ZooKeeper 服务停止运作。 现在，我们已经清楚地了解到，节点数量应该是奇数，让我们再来思考一下 ZooKeeper 集合体中需要有多少个节点。读取操作始终从连接到客户端的 ZooKeeper 服务器读取数据，所以它们的性能不会随着集合体中的服务器数量额变化而变化。但是，仅在写入法定数量的节点时，写入操作才是成功的。这意味着，随着在集合体中的节点数量的增加，写入性能会下降，因为必须将写入内容写入到更多的服务器中，并在更多服务器之间进行协调。 ZooKeeper的美妙之处在于，想运行多少服务器完全由您自己决定。如果想运行一台服务器，从 ZooKeeper的角度来看是没问题的；只是您的系统不再是高度可靠或高度可用的。三个节点的 ZooKeeper 集合体支持在一个节点故障的情况下不丢失服务，这对于大多数用户而言，这可能是没问题的，也可以说是最常见的部署拓扑。不过，为了安全起见，可以在您的集合体中使用五个节点。五个节点的集合体让您可以拿出一台服务器进行维护或滚动升级，并能够在不中断服务的情况下承受第二台服务器的意外故障。 因此，在 ZooKeeper 集合体中，三、五或七是最典型的节点数量。请记住，ZooKeeper 集合体的大小与分布式系统中的节点大小没有什么关系。分布式系统中的节点将是 ZooKeeper 集合体的客户端，每个 ZooKeeper服务器都能够以可扩展的方式处理大量客户端。例如，HBase（Hadoop 上的分布式数据库）依赖​​于 ZooKeeper 实现区域服务器的领导者选举和租赁管理。您可以利用一个相对较少（比如说，五个）节点的 ZooKeeper 集合体运行有 50 个节点的大型 HBase 集群。 设置并部署 ZooKeeper 集合体现在让我们设置并部署有三个节点的 ZooKeeper集合体。在这里，我们将使用撰写本文时的最新版的 ZooKeeper：3.4.5。我们用于此演示的节点被命名为zkserver1.mybiz.com、zkserver2.mybiz.com和zk3server3.mybiz.com。必须在每个节点上遵循下面的步骤来启动 ZooKeeper 服务器： 1.如果尚未安装 JDK，请下载安装它。这是必需的，因为 ZooKeeper 服务器在 JVM 上运行。2.下载 ZooKeeper 3.4.5. tar.gz tarball 并将它解压缩到适当的位置。清单 1. 下载 ZooKeeper tarball 并将它解压缩到适当的位置123 wgethttp://www.bizdirusa.com/mirrors/apache/ZooKeeper/stable/zookeeper3.4.5.tar.gz tar xzvf zookeeper3.4.5.tar.gz 3.创建一个目录，用它来存储与 ZooKeeper 服务器有关联的一些状态：mkdir /var/lib/zookeeper。您可能需要将这个目录创建为根目录，并在以后将这个目录的所有者更改为您希望运行ZooKeeper服务器的用户。4.设置配置。创建或编辑zookeeper3.4.5/conf/zoo.cfg文件，使其与 清单 2 相似。清单 2. 设置配置12345678tickTime=2000dataDir=/var/lib/zookeeper clientPort=2181initLimit=5 syncLimit=2server.1=zkserver1.mybiz.com:2888:3888server.2=zkserver2.mybiz.com:2888:3888server.3=zkserver3.mybiz.com:2888:3888 值得重点注意的一点是，所有三个机器都应该打开端口 2181、2888 和 3888。在本例中，端口 2181 由 ZooKeeper 客户端使用，用于连接到 ZooKeeper 服务器；端口 2888 由对等 ZooKeeper 服务器使用，用于互相通信；而端口 3888 用于领导者选举。您可以选择自己喜欢的任何端口。通常建议在所有 ZooKeeper 服务器上使用相同的端口。 5.创建一个 /var/lib/zookeeper/myid文件。此文件的内容将只包含 zkserver1.mybiz.com 上的数字 1、zkserver2.mybiz.com 上的数字 2 和 zkserver3.mybiz.com 上的数字 3。清单 3 显示了来自 zkserver1.mybiz.com 的此文件的 cat 输出。清单 3. cat 输出12mark@zkserver1.mybiz.com:~# cat/var/lib/zookeeper/myid 1 现在，您已经做好了在每台机器上启动 ZooKeeper 服务器的准备。 清单 4. 启动 ZooKeeper 服务器 bin/zkServer.sh start 现在，您可以从其中一台正在运行 ZooKeeper 服务器的机器上启动一个 CLI 客户端。 清单 5. 启动 CLI 客户端12zookeeper3.4.5/ bin/zkCli.sh serverzkserver1.mybiz.com:2181,zkserver2.mybiz.com:2181,zkserver3.mybiz.com:2181 客户端提供一个服务器列表，可以任意选中一个进行连接。如果在连接过程中失去与该服务器的连接，则会选中列表中的另一台服务器，而且客户端会话也会转移到该服务器。一旦启动了客户端，您就可以创建、编辑和删除 znode。让我们在 /mynode 创建一个znode，使用helloworld作为关联的数据。 清单 6. 在 /mynode 上创建一个 znode12[zk:127.0.0.1:2181(CONNECTED) 2] create /mynodehelloworld Created /mynode 现在，让我们在 /mynode 验证和检索数据。 清单 7. 在 /mynode 验证和检索数据1234567[zk:127.0.0.1:2181(CONNECTED) 6] get /mynodehelloworld cZxid = 0x200000005 ctime = Sat Jul 2019:53:52 PDT 2013 mZxid = 0x200000005 mtime = SatJul 20 19:53:52 PDT 2013 pZxid = 0x200000005cversion = 0 dataVersion = 0 aclVersion = 0ephemeralOwner = 0x0 dataLength = 11 numChildren =0 您会发现，在获取一个 znode 数据时，客户端也返回了一些与 znode 有关的元数据。此元数据中的一些重要字段包括，与创建和最后修改 znode 的时间有关的阶段时间戳（ctime 和 mtime）、每次修改数据都会更改的数据版本（dataVersion）、数据长度（dataLength）、这个 znode 的子节点的数量（numChildren）。我们现在可以删除 znode。 清单 8. 删除 znode12 [zk:127.0.0.1:2181(CONNECTED) 7]rmr /mynode 让我们在 /mysecondnode 创建另一个 znode。 清单 9. 创建另一个 znode```[zk:127.0.0.1:2181(CONNECTED) 10] create/mysecondnode hello Created /mysecondnode现在，让我们在 /mysecondnode 验证和检索数据。这一次，我们在最后提供了一个可选参数 1。此参数为 /mysecondnode 上的数据设置了一个一次性的触发器（名称为 watch）。如果另一个客户端在 /mysecondnode 上修改数据，该客户端将会获得一个异步通知。请注意，该通知只发送一次，除非 watch 被重新设置，否则不会因数据发生改变而再次发送通知。 清单 10. 在 /mysecondnode 上验证和检索数据 [zk:127.0.0.1:2181(CONNECTED) 12] get/mysecondnode 1 hello cZxid = 0x200000007 ctime =Sat Jul 20 19:58:27 PDT 2013 mZxid = 0x200000007mtime = Sat Jul 20 19:58:27 PDT 2013 pZxid =0x200000007 cversion = 0 dataVersion = 0aclVersion = 0 ephemeralOwner = 0x0 dataLength = 5numChildren = 0现在，从不同的客户端（比如，从不同的机器）更改与 /mysecondnode 有关联的数据的值。 清单 11. 更改与 /mysecondnode 有关联的数据的值 [zk: localhost:2181(CONNECTED)1] set /mysecondnode hello2 cZxid = 0x200000007ctime = Sat Jul 20 19:58:27 PDT 2013 mZxid =0x200000009 mtime = Sat Jul 20 20:02:37 PDT 2013pZxid = 0x200000007 cversion = 0 dataVersion = 1aclVersion = 0 ephemeralOwner = 0x0 dataLength = 6numChildren = 0您会发现，在第一个客户端上获得了一个 watch 通知。 清单 12. 在第一个客户端上获得了一个 watch 通知 [zk:127.0.0.1:2181(CONNECTED) 13] WATCHER::WatchedEvent state:SyncConnectedtype:NodeDataChanged path:/mysecondnode继续下去，因为 znode 形成了一个分层命名空间，所以您还可以创建子节点。 清单 13. 创建子节点 [zk:localhost:2181(CONNECTED) 2] create /mysecondnode/subnode 123 Created /mysecondnode/ subnode您可以获得关于某个 znode 的其他统计元数据。 清单 14. 获得关于某个 znode 的其他统计元数据 [zk:127.0.0.1:2181(CONNECTED)14] stat /mysecondnode cZxid = 0x200000007 ctime =Sat Jul 20 19:58:27 PDT 2013 mZxid = 0x200000009mtime = Sat Jul 20 20:02:37 PDT 2013 pZxid =0x20000000a cversion = 1 dataVersion = 1aclVersion = 0 ephemeralOwner = 0x0 dataLength = 6numChildren = 1在上面的示例中，我们使用了 ZooKeeper 的 CLI 客户端与 ZooKeeper 服务器进行交互。ZooKeeper 提供了 Java™、C、Python 和其他绑定。您可以通过这些绑定调用客户端 API，将 Java、C 或 Python 应用程序转换为 ZooKeeper 客户端。 ZooKeeper 的应用程序 由于 ZooKeeper 在分布式系统中提供了一些多功能的用例，ZooKeeper 有一组不同的实用应用程序。我们将在这里列出部分这些应用程序。这些应用程序大多取自 Apache ZooKeeper 维基，那里还提供了一个更完整的最新列表。请参阅 参考资料，获得这些技术的链接： Apache Hadoop 依靠 ZooKeeper 来实现 Hadoop HDFS NameNode 的自动故障转移，以及 YARN ResourceManager 的高可用性。 Apache HBase 是构建于 Hadoop 之上的分布式数据库，它使用 ZooKeeper 来实现区域服务器的主选举（master election）、租赁管理以及区域服务器之间的其他通信。 Apache Accumulo 是构建于 Apache ZooKeeper（和 Apache Hadoop）之上的另一个排序分布式键/值存储。 Apache Solr 使用 ZooKeeper 实现领导者选举和集中式配置。 Apache Mesos 是一个集群管理器，提供了分布式应用程序之间高效的资源隔离和共享。Mesos 使用 ZooKeeper 实现了容错的、复制的主选举。 Neo4j 是一个分布式图形数据库，它使用 ZooKeeper 写入主选择和读取从协调（read slave coordination）。 Cloudera Search 使用 ZooKeeper（通过 Apache Solr）集成了搜索功能与 Apache Hadoop，以实现集中式配置管理。 结束语 实现您自己的协议来协调分布式系统，这可能是一个令人感到沮丧的费时的过程。这正是 ZooKeeper 发挥其作用的地方。ZooKeeper 是一个稳定的、简单的、高性能的协调服务，为您提供编写正确的分布式应用程序所需的工具，而无需担心竞争条件、死锁和不一致。在下一次编写分布式应用程序时，您就可以利用 ZooKeeper 支持所有协调需求。","tags":[{"name":"ZooKeeper","slug":"ZooKeeper","permalink":"http://jishusuishouji.github.io/tags/ZooKeeper/"}]},{"title":"Pointfree 编程风格指南","date":"2017-03-25T15:21:00.000Z","path":"2017/03/25/hanshushi/Pointfree_编程风格指南/","text":"函数式编程有什么用？Pointfree就是如何使用函数式编程的答案。 一、程序的本质左侧是数据输入（input），中间是一系列的运算步骤，对数据进行加工，右侧是最后的数据输出（output）。一个或多个这样的任务，就组成了程序。输入和输出（统称为 I/O）与键盘、屏幕、文件、数据库等相关。这里的关键是，中间的运算部分不能有 I/O 操作，应该是纯运算，即通过纯粹的数学运算来求值。否则，就应该拆分出另一个任务。I/O 操作往往有现成命令，大多数时候，编程主要就是写中间的那部分运算逻辑。现在，主流写法是过程式编程和面向对象编程，但是我觉得，最合适纯运算的是函数式编程。 二、函数的拆分与合成上面那张图中，运算过程可以用一个函数fn表示。fn的类型如下。1fn :: a -&gt; b 上面的式子表示，函数fn的输入是数据a，输出是数据b。如果运算比较复杂，通常需要将fn拆分成多个函数。f1、f2、f3的类型如下。123f1 :: a -&gt; mf2 :: m -&gt; nf3 :: n -&gt; b 上面的式子中，输入的数据还是a，输出的数据还是b，但是多了两个中间值m和n。我们可以把整个运算过程，想象成一根水管（pipe），数据从这头进去，那头出来。 函数的拆分，无非就是将一根水管拆成了三根。 进去的数据还是a，出来的数据还是b。fn与f1、f2、f3的关系如下。1fn = R.pipe(f1, f2, f3); 上面代码中，我用到了Ramda函数库的pipe方法，将三个函数合成为一个。 三、Pointfree 的概念1fn = R.pipe(f1, f2, f3); 这个公式说明，如果先定义f1、f2、f3，就可以算出fn。整个过程，根本不需要知道a或b。也就是说，我们完全可以把数据处理的过程，定义成一种与参数无关的合成运算。不需要用到代表数据的那个参数，只要把一些简单的运算步骤合成在一起即可。这就叫做Pointfree：不使用所要处理的值，只合成运算过程。中文可以译作”无值”风格。请看下面的例子。12var addOne = x =&gt; x + 1;var square = x =&gt; x * x; 上面是两个简单函数addOne和square。把它们合成一个运算。123var addOneThenSquare = R.pipe(addOne, square);addOneThenSquare(2) // 9 上面代码中，addOneThenSquare是一个合成函数。定义它的时候，根本不需要提到要处理的值，这就是Pointfree。 四、Pointfree 的本质Pointfree的本质就是使用一些通用的函数，组合出各种复杂运算。上层运算不要直接操作数据，而是通过底层函数去处理。这就要求，将一些常用的操作封装成函数。比如，读取对象的role属性，不要直接写成obj.role，而是要把这个操作封装成函数。12var prop = (p, obj) =&gt; obj[p];var propRole = R.curry(prop)(&apos;role&apos;); 上面代码中，prop函数封装了读取操作。它需要两个参数p（属性名）和obj（对象）。这时，要把数据obj要放在最后一个参数，这是为了方便柯里化。函数propRole则是指定读取role属性。12345678910111213var isWorker = s =&gt; s === &apos;worker&apos;;var getWorkers = R.filter(R.pipe(propRole, isWorker));var data = [ &#123;name: &apos;张三&apos;, role: &apos;worker&apos;&#125;, &#123;name: &apos;李四&apos;, role: &apos;worker&apos;&#125;, &#123;name: &apos;王五&apos;, role: &apos;manager&apos;&#125;,];getWorkers(data)// [// &#123;&quot;name&quot;: &quot;张三&quot;, &quot;role&quot;: &quot;worker&quot;&#125;,// &#123;&quot;name&quot;: &quot;李四&quot;, &quot;role&quot;: &quot;worker&quot;&#125;// ] 上面代码中，data是传入的值，getWorkers是处理这个值的函数。定义getWorkers的时候，完全没有提到data，这就是Pointfree。简单说，Pointfree就是运算过程抽象化，处理一个值，但是不提到这个值。这样做有很多好处，它能够让代码更清晰和简练，更符合语义，更容易复用，测试也变得轻而易举。 五、Pointfree 的示例一下面，我们来看一个示例。1var str = &apos;Lorem ipsum dolor sit amet consectetur adipiscing elit&apos;; 上面是一个字符串，请问其中最长的单词有多少个字符？我们先定义一些基本运算。123456789101112131415// 以空格分割单词var splitBySpace = s =&gt; s.split(&apos; &apos;);// 每个单词的长度var getLength = w =&gt; w.length;// 词的数组转换成长度的数组var getLengthArr = arr =&gt; R.map(getLength, arr); // 返回较大的数字var getBiggerNumber = (a, b) =&gt; a &gt; b ? a : b;// 返回最大的一个数字var findBiggestNumber = arr =&gt; R.reduce(getBiggerNumber, 0, arr); 然后，把基本运算合成为一个函数（查看完整代码）。1234567var getLongestWordLength = R.pipe( splitBySpace, getLengthArr, findBiggestNumber);getLongestWordLength(str) // 11 可以看到，整个运算由三个步骤构成，每个步骤都有语义化的名称，非常的清晰。这就是 Pointfree 风格的优势。Ramda 提供了很多现成的方法，可以直接使用这些方法，省得自己定义一些常用函数。123456// 上面代码的另一种写法var getLongestWordLength = R.pipe( R.split(&apos; &apos;), R.map(R.length), R.reduce(R.max, 0)); 六、Pointfree 示例二下面是一段服务器返回的 JSON 数据。现在要求是，找到用户 Scott 的所有未完成任务，并按到期日期升序排列。 过程式编程的代码如下:上面代码不易读，出错的可能性很大。现在使用 Pointfree 风格改写。12345678var getIncompleteTaskSummaries = function(membername) &#123; return fetchData() .then(R.prop(&apos;tasks&apos;)) .then(R.filter(R.propEq(&apos;username&apos;, membername))) .then(R.reject(R.propEq(&apos;complete&apos;, true))) .then(R.map(R.pick([&apos;id&apos;, &apos;dueDate&apos;, &apos;title&apos;, &apos;priority&apos;]))) .then(R.sortBy(R.prop(&apos;dueDate&apos;)));&#125;; 上面代码已经清晰很多了。另一种写法是，把各个then里面的函数合成起来。12345678910111213141516171819202122232425262728293031// 提取 tasks 属性var SelectTasks = R.prop(&apos;tasks&apos;);// 过滤出指定的用户var filterMember = member =&gt; R.filter( R.propEq(&apos;username&apos;, member));// 排除已经完成的任务var excludeCompletedTasks = R.reject(R.propEq(&apos;complete&apos;, true));// 选取指定属性var selectFields = R.map( R.pick([&apos;id&apos;, &apos;dueDate&apos;, &apos;title&apos;, &apos;priority&apos;]));// 按照到期日期排序var sortByDueDate = R.sortBy(R.prop(&apos;dueDate&apos;));// 合成函数var getIncompleteTaskSummaries = function(membername) &#123; return fetchData().then( R.pipe( SelectTasks, filterMember(membername), excludeCompletedTasks, selectFields, sortByDueDate, ) );&#125;; 上面的代码跟过程式的写法一比较，孰优孰劣一目了然。 函数式编程函数式编程是一种编程的模式，在这种编程模式中最常用的函数和表达式。它强调在编程的时候用函数的方式思考问题，函数也与其他数据类型一样，处于平等地位。可以将函数作为参数传入另一个函数，也可以作为别的函数的返回值。函数式编程倾向于用一系列嵌套的函数来描述运算过程。","tags":[{"name":"函数式编程","slug":"函数式编程","permalink":"http://jishusuishouji.github.io/tags/函数式编程/"}]},{"title":"用Nginx搭建CDN服务器方法-开启Nginx缓存与镜像,自建图片服务器","date":"2017-03-23T15:06:20.000Z","path":"2017/03/23/nginx/用Nginx搭建CDN服务器方法-开启Nginx缓存与镜像,自建图片服务器/","text":"Nginx的proxy_cache和proxy_store很强大，利用proxy_store搭建图片服务器镜像实际上就相当于七牛和又拍的镜像CDN功能了，自动拉取图片保存在CDN服务器上。而proxy_cache作为Nginx缓存，既可以用作负载均衡，也可以反向绑定域名。 用Nginx搭建CDN服务器方法-开启Nginx缓存与镜像,自建图片服务器一、利用Nginx的proxy_cache搭建缓存服务器一：编译ngx_cache_purge1、Nginx的Proxy_cache是根据Key值md5哈希存储缓存，支持任意的Key，例如你可以根据”域名、URI、参数”组合成key，也支持非200状态码，如404/302等。2、要利用Nginx的Proxy_cache，你需要在Nginx编译进ngx_cache_purge 模块，执行：nginx -V，查看有没有ngx_cache_purge字样，没有的话需要自己手动编译。 3、这里以Oneinstack编译ngx_cache_purge模块作为操作演示，如果你用的是其它的LNMP包可以参考，基本过程是差不多的。命令如下：123456789101112131415161718cd /root/oneinstack/src #进入安装包目录nginx -Vtar xzf nginx-1.10.3.tar.gz #根据上面查看到的nginx版本选择解压包 wget http://labs.frickle.com/files/ngx_cache_purge-2.3.tar.gztar zxvf ngx_cache_purge-2.3.tar.gzcd /root/oneinstack/src/nginx-1.10.3 # 下面的./configure 后加的参数，你可以直接复制刚刚用nginx -V得到的参数，然后在最后加上--add-module=../ngx_cache_purge-2.3即可，参考：./configure --prefix=/usr/local/nginx --user=www --group=www --with-http_stub_status_module --with-http_v2_module --with-http_ssl_module --with-http_gzip_static_module --with-http_realip_module --with-http_flv_module --with-http_mp4_module --with-openssl=../openssl-1.0.2k --with-pcre=../pcre-8.39 --with-pcre-jit --with-ld-opt=-ljemalloc --add-module=../ngx_cache_purge-2.3 make mv /usr/local/nginx/sbin/nginx&#123;,$(date +%m%d)&#125;cp objs/nginx /usr/local/nginx/sbin #oneinstack，其它的可以不用这个操作 nginx -tservice nginx restart 4、安装完成后，再次nginx -V你就可以看到Nginx已经成功编译进了ngx_cache_purge 了。 二、利用Nginx的proxy_cache搭建缓存服务器二：修改Nginx配置文件1、先找到你的Nginx配置文件：nginx.conf（路径一般是在/usr/local/nginx/conf/nginx.conf），在配置文件Http中加入以下代码：（注意修改路径为你自己的路径）123456789proxy_connect_timeout 5;proxy_read_timeout 60;proxy_send_timeout 5;proxy_buffer_size 16k;proxy_buffers 4 64k;proxy_busy_buffers_size 128k;proxy_temp_file_write_size 128k;proxy_cache_path /data/wwwroot/pic.test.com levels=1:2 keys_zone=cache_one:200m inactive=30d max_size=5g;proxy_temp_path /data/wwwroot/pic.test.com/temp; 2、操作如下图：Nginx搭建CDN添加代码3、然后在你的虚拟主机的nginx.conf（路径一般是/usr/local/nginx/conf/vhost/pic.freehao123.com.conf），在server listen 80 和 listen 443 ssl http2 都加入下面命令：12345678910111213 location /{ proxy_pass https://www.freehao123.com; proxy_redirect off; proxy_set_header Host www.freehao123.com; proxy_cache cache_one; proxy_cache_valid 200 302 304 365d; proxy_cache_valid 301 1d; proxy_cache_valid any 1m; add_header Images-Cache “$upstream_cache_status from $host”; add_header Pragma public; add_header Cache-Control “public, must-revalidate, proxy-revalidate”; access_log off; log_not_found off; expires max;}4、将配置文件保存重新上传,然后执行:12nginx -tservice nginx restart5、先执行检查Nginx配置是否正确，确认没有问题的就是重启Nginx了。Nginx搭建CDN重启服务器6、如果你想缓存gravatar头像，那么代码就是：12345678910111213 location /avatar{ proxy_pass http://cn.gravatar.com; proxy_redirect off; proxy_set_header Host cn.gravatar.com; proxy_cache cache_one; proxy_cache_valid 200 302 304 365d; proxy_cache_valid 301 1d; proxy_cache_valid any 1m; add_header Images-Cache “$upstream_cache_status from $host”; add_header Pragma public; add_header Cache-Control “public, must-revalidate, proxy-revalidate”; access_log off; log_not_found off; expires max;}7、现在打开你的二级域名：pic.freehao123.com，你就可以看到已经正确缓存了图片了。Nginx搭建CDN缓存头像8、这里再给出另一个Nginx缓存代码，实现效果和上面是一样的。123456789101112131415161718192021222324 #先在Nginx配置中写入以下命令：proxy_temp_file_write_size 128k;proxy_temp_path /data/wwwroot/pic.ucblog.net/temp;proxy_cache_path /data/wwwroot/pic.ucblog.net levels=1:2 keys_zone=cache_one:500m inactive=7d max_size=5g; #再在虚拟主机的Nginx配置中写入以下命令：先在server listen 80 和listen 443代码前面加入：upstream gravatar { server secure.gravatar.com:443;}再在server listen 80 和listen 443 里面加入：location / { proxy_pass_header Server; proxy_set_header Host cn.gravatar.com; proxy_set_header Accept-Encoding ‘’; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass https://gravatar; proxy_cache cache_one; proxy_cache_valid 200 304 365d; proxy_cache_key $host$uri$is_args$args; expires max; }9、在VPS主机上，你可以看到proxy_cache生成的哈希文件，就表示缓存已经成功了。Nginx搭建CDN生成缓存文件三、利用Nginx的proxy_store搭建镜像服务器：修改Nginx配置方法1、Nginx的proxy_store作用是直接把静态文件在本地硬盘创建并读取，类似于七牛或者又拍这样的镜像CDN功能，首次访问会自动获取源站的静态图片等文件，之后的访问就是直接从CDN服务器读取，加快了速度。2、直接修改Nginx的虚拟主机配置文件（这里以img.freehao123.com.conf为演示），加入以下代码：1234567891011location / { expires 3d; proxy_set_header Accept-Encoding ‘’; root /data/wwwroot/img.freehao123.com; proxy_store on; proxy_store_access user:rw group:rw all:rw; proxy_temp_path /data/wwwroot/img.freehao123.com/temp; if ( !-e $request_filename) { proxy_pass https://www.freehao123.com; } }3、再次保存配置上传，然后重启Nginx。你可以看到img.freehao123.com请求的图片等静态文件已经成功从源站中获得到了。Nginx搭建CDN图片请求4、在VPS主机上的存目录中也可以看到proxy_store已经完整地将图片等静态文件的目录都保存下来了，相当于一个网站的镜像存储CDN了。Nginx搭建CDN镜像存储5、这里还有一个使用，效果和上面是一样的，记得替换好路径，代码如下：12345678910111213141516upstream http_tornado { server www.freehao123.com:443;} server { # 省略其他配置 location ~ .*\\.(gif|jpg|jpeg|png|bmp|swf|js|html|htm|css)$ { root /opt/data/product/blog/cache; proxy_store on; proxy_store_access user:rw group:rw all:rw; proxy_temp_path /opt/data/product/blog/cache; if ( !-e $request_filename) { proxy_pass http://http_tornado; } } }四、Nginx的proxy_store和proxy_cache有什么区别？1、镜像与缓存的区别。从上面的介绍我们也可以看出来，proxy_store相当于镜像一个网站了，第二次访问图片等静态文件是直接读取CDN服务器上的，大大减轻了源站的负担。proxy_cache相当于缓存，即把请求生成Key，第二次访问就可以加快速度了。Nginx搭建CDN加快速度2、proxy_store适合静态，proxy_cache适合动态。proxy_store是将图片完整保存在CDN服务器上，所以它更适合于图片CDN加速，而proxy_cache是缓存生成Key，更加适合动态网站加速，可用于负载均衡，减轻服务器负担。Nginx搭建CDN减轻负担五、搭建镜像CDN服务器后要做的事情？1、第一，因为搭建镜像CDN服务器是完整地复制了源站的文件和URL，所以为了避免被搜索引擎误认为抄袭重复站，我们可以给CDN站加上Robots.txt，阻止搜索引擎收录。命令如下（允许收录图片，其它不允许爬取）：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192User-agent: BaiduspiderAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: 360SpiderAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: Baiduspider-imageAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: 360Spider-ImageAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: SosospiderAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: sogou spiderAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: YodaoBotAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: GooglebotAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: BingbotAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: SlurpAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: MSNBotAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: googlebot-imageAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: googlebot-mobileAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: yahoo-blogs/v3.9Allow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: psbotAllow: /wp-content/uploads/.jpg$Allow: /wp-content/uploads/.png$Allow: /wp-content/uploads/*.gif$Disallow: / User-agent: Disallow: /2、第二，做好Nginx防盗链。如果你的CDN服务器流量不怎么够的话，建议还是做好防盗链措施，同时还可以帮你减轻服务器负担。在你的虚拟主机配置文件中加入以下代码：12345678location ~ ..(gif|jpg|jpeg|png|bmp|swf)$ { valid_referers none blocked freehao123.com .freehao123.com .google.cn .google.com .google.com.hk image.baidu.com *.baidu.com; if ($invalid_referer) { rewrite ^/ https://www.freehao123.com; #return 403; } } 3、第三，设置好Nginx默认图片。这个主要是针对缓存Gravatar头像的，当源站服务器不存在某一个图片或者文件时，我们可以给Nginx设置一个默认的图片或者链接，这样缓存看起来就完美了。123456789101112 location /avatar { try_files $uri /avatar/set-avatar.png; } #或者使用： location /{ try_files $uri /set-avatar.png; }4、效果见下图：用Nginx搭建CDN默认图片文章出自：免费资源部落 部分内容参考张戈博客\\cheyo.net\\ttt.tt版权所有。本站文章除注明出处外，皆为作者原创文章，可自由引用，但请注明来源。2014年六大免费VPS主机-免费VPS申请、使用和点评您或许对下面这些文章有兴趣: 本月吐槽辛苦排行榜UPyun又拍云CDN安装部署Let’s Encrypt免费SSL证书和配置自定义SSL证书Kloudsec免费CDN加速-提供免费SSL证书支持Https自定义SSL新加坡节点服务器性能管理(APM)：性能魔方mmtrix一站式云评测,云监测,云加速网站UPYUN又拍云动态CDN和静态CDN加速支持自定义域名Https和图片处理阿里百川多媒体-20GB免费存储空间和CDN流量支持图片,视频在线处理2014年十个优秀的免费CDN加速服务-国内和国外免费CDNIncapsula免费CDN服务申请使用:日本,香港,美国CDN加速效果测评Discuz论坛使用七牛,又拍,阿里云OSS CDN加速：CSS,JS,图片,论坛附件","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://jishusuishouji.github.io/tags/Nginx/"}]},{"title":"druid 配置WebStatFilter 网络url统计","date":"2017-03-23T14:54:59.000Z","path":"2017/03/23/druid/druid_配置WebStatFilter_网络url统计/","text":"WebStatFilter用于采集web-jdbc关联监控的数据。 web.xml配置123456789101112&lt;filter&gt; &lt;filter-name&gt;DruidWebStatFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.alibaba.druid.support.http.WebStatFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;exclusions&lt;/param-name&gt; &lt;param-value&gt;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;DruidWebStatFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; exlusions配置经常需要排除一些不必要的url，比如.js,/jslib/等等。配置在init-param中。比如：1234&lt;init-param&gt; &lt;param-name&gt;exclusions&lt;/param-name&gt; &lt;param-value&gt;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid/*&lt;/param-value&gt; &lt;/init-param&gt; sessionStatMaxCount配置缺省sessionStatMaxCount是1000个。你可以按需要进行配置，比如：1234&lt;init-param&gt; &lt;param-name&gt;sessionStatMaxCount&lt;/param-name&gt; &lt;param-value&gt;1000&lt;/param-value&gt; &lt;/init-param&gt; `sessionStatEnable配置你可以关闭session统计功能，比如：1234&lt;init-param&gt; &lt;param-name&gt;sessionStatEnable&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; principalSessionName配置你可以配置principalSessionName，使得druid能够知道当前的session的用户是谁。比如：1234&lt;init-param&gt; &lt;param-name&gt;principalSessionName&lt;/param-name&gt; &lt;param-value&gt;xxx.user&lt;/param-value&gt; &lt;/init-param&gt; 根据需要，把其中的xxx.user修改为你user信息保存在session中的sessionName。 注意：如果你session中保存的是非string类型的对象，需要重载toString方法 principalCookieName如果你的user信息保存在cookie中，你可以配置principalCookieName，使得druid知道当前的user是谁1234&lt;init-param&gt; &lt;param-name&gt;principalCookieName&lt;/param-name&gt; &lt;param-value&gt;xxx.user&lt;/param-value&gt; &lt;/init-param&gt; 根据需要，把其中的xxx.user修改为你user信息保存在cookie中的cookieName profileEnabledruid 0.2.7版本开始支持profile，配置profileEnable能够监控单个url调用的sql列表。1234&lt;init-param&gt; &lt;param-name&gt;profileEnable&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;","tags":[{"name":"druid","slug":"druid","permalink":"http://jishusuishouji.github.io/tags/druid/"}]},{"title":"DRUID连接池的实用 配置详解","date":"2017-03-23T14:03:57.000Z","path":"2017/03/23/druid/DRUID连接池的实用_配置详解/","text":"DRUID介绍DRUID是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、PROXOOL等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池(据说是目前最好的连接池,不知道速度有没有BoneCP快)。 配置参数和其它连接池一样DRUID的DataSource类为：com.alibaba.druid.pool.DruidDataSource，基本配置参数如下： name 配置这个属性的意义在于，如果存在多个数据源，监控的时候可以通过名字来区分开来。如果没有配置，将会生成一个名字，格式是：”DataSource-“ + System.identityHashCode(this) jdbcUrl连接数据库的url，不同数据库不一样。例如：mysql:jdbc:mysql://10.20.153.104:3306/druid2oracle:jdbc:oracle:thin:@10.20.149.85:1521:ocnauto username连接数据库的用户名 password连接数据库的密码。如果你不希望密码直接写在配置文件中，可以使用ConfigFilter。 driverClassName根据url自动识别这一项可配可不配，如果不配置druid会根据url自动识别dbType，然后选择相应的driverClassName(建议配置下) initialSize默认值0，初始化时建立物理连接的个数。初始化发生在显示调用init方法，或者第一次getConnection时 maxActive默认值8最大连接池数量 maxIdle默认值8已经不再使用，配置了也没效果 minIdle最小连接池数量 maxWait获取连接时最大等待时间，单位毫秒。配置了maxWait之后，缺省启用公平锁，并发效率会有所下降，如果需要可以通过配置useUnfairLock属性为true使用非公平锁。 poolPreparedStatementsfalse是否缓存preparedStatement，也就是PSCache。PSCache对支持游标的数据库性能提升巨大，比如说oracle。在mysql下建议关闭。 maxOpenPreparedStatements-1要启用PSCache，必须配置大于0，当大于0时，poolPreparedStatements自动触发修改为true。在Druid中，不会存在Oracle下PSCache占用内存过多的问题，可以把这个数值配置大一些，比如说100 validationQuery用来检测连接是否有效的sql，要求是一个查询语句。如果validationQuery为null，testOnBorrow、testOnReturn、testWhileIdle都不会其作用。 testOnBorrowtrue申请连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能。 testOnReturnfalse归还连接时执行validationQuery检测连接是否有效，做了这个配置会降低性能 testWhileIdlefalse建议配置为true，不影响性能，并且保证安全性。申请连接的时候检测，如果空闲时间大于timeBetweenEvictionRunsMillis，执行validationQuery检测连接是否有效。timeBetweenEvictionRunsMillis有两个含义：1) Destroy线程会检测连接的间隔时间2) testWhileIdle的判断依据，详细看testWhileIdle属性的说明 numTestsPerEvictionRun不再使用，一个DruidDataSource只支持一个EvictionRun minEvictableIdleTimeMillisconnectionInitSqls物理连接初始化的时候执行的sql exceptionSorter根据dbType自动识别当数据库抛出一些不可恢复的异常时，抛弃连接 filters属性类型是字符串，通过别名的方式配置扩展插件，常用的插件有：监控统计用的filter:stat日志用的filter:log4j防御sql注入的filter:wall proxyFilters类型是List&lt;com.alibaba.druid.filter.Filter&gt;，如果同时配置了filters和proxyFilters，是组合关系，并非替换关系 使用方法DB数据源的使用方法也就是2种，一种是在代码中写死通过NEW操作符创建DataSSource，然后set一些连接属性;另外一种是基于SPRING的配置方法，然后让SPRING的Context自动加载配置（以下配置文件默认都在项目根目录下conf文件夹中） 1、属性文件:application.properties(DataSource连接参数) 1234jdbc.driverClassName=com.mysql.jdbc.Driver jdbc.url=jdbc:mysql://127.0.0.1:3306/test jdbc.username=root jdbc.password=1qaz!QAZ 2、SPRING配置文件：spring-base.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot; http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot; http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:batch=&quot; http://www.springframework.org/schema/batch&quot; xsi:schemaLocation=&quot; http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.0.xsd&quot;&gt; &lt;bean id=&quot;propertyConfigure&quot; class=&quot;org.springframework.beans.factory.config.PropertyPlaceholderConfigurer&quot;&gt; &lt;property name=&quot;locations&quot;&gt; &lt;list&gt; &lt;value&gt;./conf/application.properties&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;dataSource&quot; class=&quot;com.alibaba.druid.pool.DruidDataSource&quot; init-method=&quot;init&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;$&#123;jdbc.driverClassName&#125;&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;$&#123;jdbc.url&#125;&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;$&#123;jdbc.username&#125;&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;jdbc.password&#125;&quot; /&gt; &lt;!-- 配置初始化大小、最小、最大 --&gt; &lt;property name=&quot;initialSize&quot; value=&quot;1&quot; /&gt; &lt;property name=&quot;minIdle&quot; value=&quot;1&quot; /&gt; &lt;property name=&quot;maxActive&quot; value=&quot;10&quot; /&gt; &lt;!-- 配置获取连接等待超时的时间 --&gt; &lt;property name=&quot;maxWait&quot; value=&quot;10000&quot; /&gt; &lt;!-- 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒 --&gt; &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot; value=&quot;60000&quot; /&gt; &lt;!-- 配置一个连接在池中最小生存的时间，单位是毫秒 --&gt; &lt;property name=&quot;minEvictableIdleTimeMillis&quot; value=&quot;300000&quot; /&gt; &lt;property name=&quot;testWhileIdle&quot; value=&quot;true&quot; /&gt; &lt;!-- 这里建议配置为TRUE，防止取到的连接不可用 --&gt; &lt;property name=&quot;testOnBorrow&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;testOnReturn&quot; value=&quot;false&quot; /&gt; &lt;!-- 打开PSCache，并且指定每个连接上PSCache的大小 --&gt; &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot; /&gt; &lt;property name=&quot;maxPoolPreparedStatementPerConnectionSize&quot; value=&quot;20&quot; /&gt; &lt;!-- 这里配置提交方式，默认就是TRUE，可以不用配置 --&gt; &lt;property name=&quot;defaultAutoCommit&quot; value=&quot;true&quot; /&gt; &lt;!-- 验证连接有效与否的SQL，不同的数据配置不同 --&gt; &lt;property name=&quot;validationQuery&quot; value=&quot;select 1 &quot; /&gt; &lt;property name=&quot;filters&quot; value=&quot;stat&quot; /&gt; &lt;property name=&quot;proxyFilters&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;logFilter&quot; /&gt; &lt;/list&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=&quot;logFilter&quot; class=&quot;com.alibaba.druid.filter.logging.Slf4jLogFilter&quot;&gt; &lt;property name=&quot;statementExecutableSqlLogEnable&quot; value=&quot;false&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; 监控方式1、WEB方式监控配置&lt;servlet&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;servlet-class&gt;com.alibaba.druid.support.http.StatViewServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;DruidStatView&lt;/servlet-name&gt; &lt;url-pattern&gt;/druid/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;druidWebStatFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.alibaba.druid.support.http.WebStatFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;exclusions&lt;/param-name&gt; &lt;param-value&gt;/public/*,*.js,*.css,/druid*,*.jsp,*.swf&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;principalSessionName&lt;/param-name&gt; &lt;param-value&gt;sessionInfo&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;profileEnable&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;druidWebStatFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 把上面servlet配置添加到项目web.xml即可。然后运行Tomcat，浏览器输入 http://IP:PROT/druid 就可以打开Druid的监控页面了. 2、日志文件监控Druid提供了多种日志文件监控commons-logging、log4j等，这里我们主要使用slf4j和logback来进行日志监控配置。 首先要引入slf4j和logback相关的jar文件（从Maven公共仓库下载 http://search.maven.org/） 12345678910111213141516171819202122232425&lt;slf4j.version&gt;1.7.7&lt;/slf4j.version&gt; &lt;logback.version&gt;1.1.2&lt;/logback.version&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;$&#123;slf4j.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-access&lt;/artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;$&#123;logback.version&#125;&lt;/version&gt; &lt;/dependency&gt; 接下配置logback的配置文件(./conf/logback.xml) 1234567891011121314151617181920212223&lt;configuration&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n &lt;/Pattern&gt; &lt;/layout&gt; &lt;/appender&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.FileAppender&quot;&gt; &lt;file&gt;./logs/druid_info.log&lt;/file&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;Pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/Pattern&gt; &lt;/layout&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;debug&lt;/level&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;root level=&quot;DEBUG&quot;&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;/root&gt; &lt;/configuration&gt; 最后就是写一个测试类进行测试 12345678910111213141516171819public class TestMain &#123; public static void loadLoggerContext() &#123; System.getProperties().put(&quot;logback.configurationFile&quot;, &quot;./conf/logback.xml&quot;); LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory(); StatusPrinter.setPrintStream(System.err); StatusPrinter.print(lc); &#125; public static void main(String[] args) &#123; try &#123; loadLoggerContext(); FileSystemXmlApplicationContext context = new FileSystemXmlApplicationContext(&quot;./conf/spring-base.xml&quot;); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125; &#125;","tags":[{"name":"Druid","slug":"Druid","permalink":"http://jishusuishouji.github.io/tags/Druid/"}]},{"title":"Druid：一个用于大数据实时处理的开源分布式系统","date":"2017-03-23T14:00:02.000Z","path":"2017/03/23/druid/Druid：一个用于大数据实时处理的开源分布式系统/","text":"Druid是一个用于大数据实时查询和分析的高容错、高性能开源分布式系统，旨在快速处理大规模的数据，并能够实现快速查询和分析。尤其是当发生代码部署、机器故障以及其他产品系统遇到宕机等情况时，Druid仍能够保持100%正常运行。创建Druid的最初意图主要是为了解决查询延迟问题，当时试图使用Hadoop来实现交互式查询分析，但是很难满足实时分析的需要。而Druid提供了以交互方式访问数据的能力，并权衡了查询的灵活性和性能而采取了特殊的存储格式。Druid功能介于PowerDrill和Dremel之间，它几乎实现了Dremel的所有功能，并且从PowerDrill吸收一些有趣的数据格式。Druid允许以类似Dremel和PowerDrill的方式进行单表查询，同时还增加了一些新特性，如为局部嵌套数据结构提供列式存储格式、为快速过滤做索引、实时摄取和查询、高容错的分布式体系架构等。从官方得知，Druid的具有以下主要特征： 为分析而设计——Druid是为OLAP工作流的探索性分析而构建，它支持各种过滤、聚合和查询等类； 快速的交互式查询——Druid的低延迟数据摄取架构允许事件在它们创建后毫秒内可被查询到； 高可用性——Druid的数据在系统更新时依然可用，规模的扩大和缩小都不会造成数据丢失； 可扩展——Druid已实现每天能够处理数十亿事件和TB级数据。 Druid应用最多的是类似于广告分析创业公司Metamarkets中的应用场景，如广告分析、互联网广告系统监控以及网络监控等。当业务中出现以下情况时，Druid是一个很好的技术方案选择： 需要交互式聚合和快速探究大量数据时；需要实时查询分析时；具有大量数据时，如每天数亿事件的新增、每天数10T数据的增加；对数据尤其是大数据进行实时分析时；需要一个高可用、高容错、高性能数据库时。一个Druid集群有各种类型的节点（Node）组成，每个节点都可以很好的处理一些的事情，这些节点包括对非实时数据进行处理存储和查询的Historical节点、实时摄取数据、监听输入数据流的Realtime节、监控Historical节点的Coordinator节点、接收来自外部客户端的查询和将查询转发到Realtime和Historical节点的Broker节点、负责索引服务的Indexer节点。 查询操作中数据流和各个节点的关系如下图所示： 如下图是Druid集群的管理层架构，该图展示了相关节点和集群管理所依赖的其他组件（如负责服务发现的ZooKeeper集群）的关系： Druid已基于Apache License 2.0协议开源，代码托管在GitHub，其当前最新稳定版本是0.7.1.1。当前，Druid已有63个代码贡献者和将近2000个关注。Druid的主要贡献者包括广告分析创业公司Metamarkets、电影流媒体网站Netflix、Yahoo等公司。Druid官方还对Druid同Shark、Vertica、Cassandra、Hadoop、Spark、Elasticsearch等在容错能力、灵活性、查询性能等方便进行了对比说明。更多关于Druid的信息，大家还可以参考官方提供的入门教程、白皮书 、设计文档等。 感谢徐川对本文的审校。","tags":[{"name":"Druid","slug":"Druid","permalink":"http://jishusuishouji.github.io/tags/Druid/"}]},{"title":"架构师必看 京东咚咚架构演进","date":"2017-03-21T13:01:44.000Z","path":"2017/03/21/jiagou/架构师必看_京东咚咚架构演进/","text":"技术架构单独拿出来看我认为没有绝对的好与不好，技术架构总是要放在彼时的背景下来看，要考虑业务的时效价值、团队的规模和能力、环境基础设施等等方面。 架构演进的生命周期适时匹配好业务的生命周期，才可能发挥最好的效果。 京东咚咚自从京东开始为第三方卖家提供入驻平台服务后，咚咚也就随之诞生了。 1.0 诞生（2010 – 2011)为了业务的快速上线，1.0 版本的技术架构实现是非常直接且简单粗暴的。 如何简单粗暴法？请看架构图，如下。京东咚咚1.0的功能十分简单，实现了一个IM的基本功能，接入、互通消息和状态。另外还有客服功能，就是顾客接入咨询时的客服分配，按轮询方式把顾客分配给在线的客服接待。 用开源Mina框架实现了TCP的长连接接入，用Tomcat Comet机制实现了HTTP的长轮询服务。而消息投递的实现是一端发送的消息临时存放在 Redis中，另一端拉取的生产消费模型。这个模型的做法导致需要以一种高频率的方式来轮询Redis遍历属于自己连接的关联会话消息。这个模型很简单，简单包括多个层面的意思：理解起来简单；开发起来简单；部署起来也简单。只需要一个Tomcat应用依赖一个共享的Redis，简单的实现核心业务功能，并支持业务快速上线。但这个简单的模型也有些严重的缺陷，主要是效率和扩展问题。轮询的频率间隔大小基本决定了消息的延时，轮询越快延时越低，但轮询越快消耗也越高。这个模型实际上是一个高功耗低效能的模型，因为不活跃的连接在那做高频率的无意义轮询。 高频有多高呢，基本在100ms以内，你不能让轮询太慢，比如超过2秒轮一次，人就会在聊天过程中感受到明显的会话延迟。 随着在线人数增加，轮询的耗时也线性增长，因此这个模型导致了扩展能力和承载能力都不好，一定会随着在线人数的增长碰到性能瓶颈。 1.0的时代背景正是京东技术平台从.NET向Java转型的年代，我也正是在这期间加入京东并参与了京东主站技术转型架构升级的过程。 之后开始接手了京东咚咚，并持续完善这个产品，进行了三次技术架构演进。 2.0 成长（2012）我们刚接手时1.0已在线上运行并支持京东POP（开放平台）业务，之后京东打算组建自营在线客服团队并落地在成都。不管是自营还是POP客服咨询业务当时都起步不久，1.0架构中的性能和效率缺陷问题还没有达到引爆的业务量级。而自营客服当时还处于起步阶段，客服人数不足，服务能力不够，顾客咨询量远远超过客服的服务能力。超出服务能力的顾客咨询，当时我们的系统统一返回提示客服繁忙，请稍后咨询。 这种状况导致高峰期大量顾客无论怎么刷新请求，都很可能无法接入客服，体验很差。所以2.0重点放在了业务功能体验的提升上，如下图所示。京东咚咚针对无法及时提供服务的顾客，可以排队或者留言。 针对纯文字沟通，提供了文件和图片等更丰富的表达方式。 另外支持了客服转接和快捷回复等方式来提升客服的接待效率。总之，整个2.0就是围绕提升客服效率和用户体验。而我们担心的效率问题在2.0高速发展业务的时期还没有出现，但业务量正在逐渐积累，我们知道它快要爆了。到2012年末，度过双十一后开始了3.0的一次重大架构升级。 3.0爆发（2013 – 2014）经历了2.0时代一整年的业务高速发展，实际上代码规模膨胀的很快。与代码一块膨胀的还有团队，从最初的4个人到近30人。 团队大了后，一个系统多人开发，开发人员层次不一，规范难统一，系统模块耦合重，改动沟通和依赖多，上线风险难以控制。一个单独tomcat应用多实例部署模型终于走到头了，这个版本架构升级的主题就是服务化。服务化的第一个问题如何把一个大的应用系统切分成子服务系统。当时的背景是京东的部署还在半自动化年代，自动部署系统刚起步，子服务系统若按业务划分太细太多，部署工作量很大且难管理。所以当时我们不是按业务功能分区服务的，而是按业务重要性级别划分了0、1、2 三个级别不同的子业务服务系统。 另外就是独立了一组接入服务，针对不同渠道和通信方式的接入端，见下图。更细化的应用服务和架构分层方式可见下图。这次大的架构升级，主要考虑了三个方面：稳定性、效率和容量。 做了下面这些事情：1.业务分级、核心、非核心业务隔离2.多机房部署，流量分流、容灾冗余、峰值应对冗余3.读库多源，失败自动转移4.写库主备，短暂有损服务容忍下的快速切换5.外部接口，失败转移或快速断路6.Redis 主备，失败转移7.大表迁移，MongoDB 取代 MySQL 存储消息记录8.改进消息投递模型 前 6 条基本属于考虑系统稳定性、可用性方面的改进升级。 这一块属于陆续迭代完成的，承载很多失败转移的配置和控制功能在上面图中是由管控中心提供的。 第 7 条主要是随着业务量的上升，单日消息量越来越大后，使用了 MongoDB来单独存储量最大的聊天记录。 第 8 条是针对 1.0版本消息轮询效率低的改进，改进后的投递方式如下图所示：不再是轮询了，而是让终端每次建立连接后注册接入点位置，消息投递前定位连接所在接入点位置再推送过去。 这样投递效率就是恒定的了，而且很容易扩展，在线人数越多则连接数越多，只需要扩展接入点即可。 其实，这个模型依然还有些小问题，主要出在离线消息的处理上，可以先思考下，我们最后再讲。3.0 经过了两年的迭代式升级，单纯从业务量上来说还可以继续支撑很长时间的增长。 但实际上到2014年底我们面对的不再是业务量的问题，而是业务模式的变化。 这直接导致了一个全新时代的到来。 4.0 涅槃（2015 至今 )2014年京东的组织架构发生了很大变化，从一个公司变成了一个集团，下设多个子公司。原来的商城成为了其中一个子公司，新成立的子公司包括京东金融、京东智能、京东到家、拍拍、海外事业部等。各自业务范围不同，业务模式也不同，但不管什么业务总是需要客服服务。如何复用原来为商城量身订做的咚咚客服系统并支持其他子公司业务快速接入成为我们新的课题。最早要求接入的是拍拍网，它是从腾讯收购的，所以是完全不同的账户和订单交易体系。由于时间紧迫，我们把为商城订做的部分剥离，基于3.0架构对接拍拍又单独订做了一套，并独立部署，像下面这样。京东咚咚虽然在业务要求的时间点前完成了上线，但这样做也带来了明显的问题：复制工程，定制业务开发，多套源码维护成本高独立部署，至少双机房主备外加一个灰度集群，资源浪费大以前我们都是面向业务去架构系统，如今新的业务变化形势下我们开始考虑面向平台去架构，在统一平台上跑多套业务，统一源码，统一部署，统一维护。 把业务服务继续拆分，剥离出最基础的 IM 服务，IM 通用服务，客服通用服务，而针对不同的业务特殊需求做最小化的定制服务开发。 部署方式则以平台形式部署，不同的业务方的服务跑在同一个平台上，但数据互相隔离。 服务继续被拆分的更微粒化，形成了一组服务矩阵（见下图）。京东咚咚而部署方式，只需要在双机房建立两套对等集群，并另外建一个较小的灰度发布集群即可，所有不同业务都运行在统一平台集群上，如下图。京东咚咚更细粒度的服务意味着每个服务的开发更简单，代码量更小，依赖更少，隔离稳定性更高。 但更细粒度的服务也意味着更繁琐的运维监控管理，直到今年公司内部弹性私有云、缓存云、消息队列、部署、监控、日志等基础系统日趋完善， 使得实施这类细粒度划分的微服务架构成为可能，运维成本可控。 而从当初 1.0 的 1 种应用进程，到 3.0 的 6、7 种应用进程，再到 4.0 的 50+ 更细粒度的不同种应用进程。 每种进程再根据承载业务流量不同分配不同的实例数，真正的实例进程数会过千。 为了更好的监控和管理这些进程，为此专门定制了一套面向服务的运维管理系统，见下图。京东咚咚统一服务运维提供了实用的内部工具和库来帮助开发更健壮的微服务。 包括中心配置管理，流量埋点监控，数据库和缓存访问，运行时隔离，如下图所示是一个运行隔离的图示：京东咚咚细粒度的微服务做到了进程间隔离，严格的开发规范和工具库帮助实现了异步消息和异步 HTTP 来避免多个跨进程的同步长调用链。 进程内部通过切面方式引入了服务增强容器 Armor 来隔离线程， 并支持进程内的单独业务降级和同步转异步化执行。而所有这些工具和库服务都是为了两个目标：让服务进程运行时状态可见让服务进程运行时状态可被管理和改变最后我们回到前文留下的一个悬念，就是关于消息投递模型的缺陷。 一开始我们在接入层检测到终端连接断开后，消息无法投递，再将消息缓存下来，等终端重连接上来再拉取离线消息。 这个模型在移动时代表现的很不好，因为移动网络的不稳定性，导致经常断链后重连。 而准确的检测网络连接断开是依赖一个网络超时的，导致检测可能不准确，引发消息假投递成功。 新的模型如下图所示，它不再依赖准确的网络连接检测，投递前待确认消息 id 被缓存，而消息体被持久存储。 等到终端接收确认返回后，该消息才算投妥，未确认的消息 id 再重新登陆后或重连接后作为离线消息推送。 这个模型不会产生消息假投妥导致的丢失，但可能导致消息重复，只需由客户终端按消息 id 去重即可。京东咚咚京东咚咚诞生之初正是京东技术转型到 Java 之时，经历这些年的发展，取得了很大的进步。 从草根走向专业，从弱小走向规模，从分散走向统一，从杂乱走向规范。 本文主要重心放在了几年来咚咚架构演进的过程，技术架构单独拿出来看我认为没有绝对的好与不好， 技术架构总是要放在彼时的背景下来看，要考虑业务的时效价值、团队的规模和能力、环境基础设施等等方面。 架构演进的生命周期适时匹配好业务的生命周期，才可能发挥最好的效果。 【编辑推荐】58同城沈剑：好的架构不是设计出来的，而是演进出来的架构必备：Rate limiting 的作用和常见方式京东11.11：商品搜索系统架构设计解密京东唐志雄：从技术角度看白条资产证券化关于Java应用相关不同产品的架构中小型网站架构分析及优化友盟吴磊：移动大数据平台的架构、实践与数据增值【责任编辑：wangxueyan TEL：（010）68476606】 点赞 0架构师 京东 架构分享:内容点评已有0条评论,0次赞还可以输入500字 请输入你的评论提交您还没有登录！请先 登录 或 注册还没有评论内容大家都在看猜你喜欢紧急预警！Struts2新漏洞S2-045来袭，多个版本受影响紧急预警！Struts2新漏洞S2-045来袭，多个版本受影响2017年，为何过半的大数据项目不成功?2017年，为何过半的大数据项目不成功?2017年3月编程语言排行榜：Swift首次进入前十2017年3月编程语言排行榜：Swift首次进入前十如何禁掉Windows 10上的所有广告如何禁掉Windows 10上的所有广告编辑推荐外电iOS与Android设备到底是如何被入侵的？头条HTML5游戏开发难点之效率、性能和加载量头条你所不了解的移动支付背后的技术支撑外电我们为何很难对超大规模应用与分布式架构进行备份？头条2017年3月编程语言排行榜：Swift首次进入前十24H热文一周话题本月最赞5个强大的Java分布式缓存框架推荐坐在马桶上看算法：快速排序Java程序员新手老手都离不开八大开发工具2017年3月编程语言排行榜：Swift首次进入前十多图详解Spring框架的设计理念与设计模式2015年十五个热门的 PHP 开发工具Java 中常用缓存Cache机制的实现浅谈Java中的Set、List、Map的区别视频课程+更多技术大咖的旅游梦：同程CTO结缘腾讯云技术大咖的旅游梦：同程CTO结缘腾讯云讲师：腾讯云1人学习过软考网络工程师考试之IP地址计算轻松解决视频课程（攻克要塞系列）软考网络工程师考试之IP地址计算轻松解决视频讲师：朱小平30人学习过大数据培训班4期培训班课程（只针对培训班学员）大数据培训班4期培训班课程（只针对培训班学讲师：徐培成0人学习过热门职位+更多后端开发全职/1-3年/大专5k-15k分享中级Java工程师全职/1-3年/大专6k-10k高达软件诚聘PHP开发师兼职/5-10年/不限15k-25k慧都科技PHP研发工程师全职/1-3年/本科10k-15k动视云科技后端开发(PHP/Go)全职/5-10年/本科30k-50k瓜子二手车最新专题+更多金三银四跳槽季 开发者这样惊呆你的面试官金三银四跳槽季 开发者这样惊呆你的面试官跳槽季你了解AJAX吗？TA不是新编程语言而是WEB应用程序技术你了解AJAX吗？TA不是新编程语言而是WEB应用程序技术AJAXWeb前端知识杂乱 如何分清主次和学习优先级？Web前端知识杂乱 如何分清主次和学习优先级？Web前端/分清主次/学习编程初学者学什么语言好？未来编程趋势预测编程初学者学什么语言好？未来编程趋势预测编程精彩评论和气高尚评论了：【51CTO学院】免费直播课 | 赵海兵–虚拟化与混合云期待中。。。！ lwt1309108评论了：【51CTO学院】免费直播课 | 赵海兵–虚拟化与混合云期待 ashely冰雪雨露评论了：【51CTO学院】免费直播课 | 赵海兵–虚拟化与混合云虚拟化对我目前的工作很要紧 Wanglican评论了：【51CTO学院】团购第二期-低至6折！名师中高级实战进阶项目交了押金了，申请进群了，麻烦通过一下。亲~~精选博文论坛热帖下载排行现阶段为开放式基金赎回良机SecureCRT 使用技巧nagios全攻略(二)—-基本安装和配置关于51CTO合作出书中的职业发展部分利用WINDOWS SERVER 2003路由设置解读 书 +更多点石成金：访客至上的网页设计秘笈（原书第2版）有些网站看起来很清爽； 有些网站看起来很杂乱； 有些网站能让你轻松地找到资料； 有些网站让你犹如置身迷宫…… … 订阅51CTO邮刊点击这里查看样刊订阅51CTO邮刊51CTO旗下网站：领先的IT技术网站 51CTO|领先的中文存储媒体 WatchStor| 中国首个CIO网站 CIOage |中国首家数字医疗网站 HC3iCopyright©2005-2017 51CTO.COM 版权所有 未经许可 请勿转载","tags":[{"name":"架构","slug":"架构","permalink":"http://jishusuishouji.github.io/tags/架构/"}]},{"title":"VAGRANT 和 Docker的使用场景和区别?","date":"2017-01-25T06:20:57.000Z","path":"2017/01/25/xunihua/VAGRANT_和_Docker的使用场景和区别_/","text":"本质区别Vagrant并不提供虚拟化技术，本质上是一个虚拟机外挂，通过虚拟机的管理接口来管理虚拟机，让用户更轻松的进行一些常用配置，比如：CPU/Memory/IP/DISK等分配。并且提供了一些其它的管理操作：比如开机运行指定命令，镜像二次打包，插件编写等等。vagrant官方有介绍: To achieve its magic, Vagrant stands on the shoulders of giants. Machines are provisioned on top of VirtualBox, VMware, AWS, or any other provider. Then, industry-standard provisioning tools such as shell scripts, Chef, or Puppet, can be used to automatically install and configure software on the machine. 而docker是一个容器引擎，每一个实例是一个相对隔离的空间，与宿主机共享操作系统内核，并且共享宿主机资源。相对于披着虚拟机皮的vagrant，docker更加轻量，消耗更少的资源。 应用场景关于应用场景没有绝对，把两个东西都用熟，自己觉得用哪个方便用哪个好管理就用哪个。既然vagrant本质是虚拟机外挂，那么它的应用场景就是，节省你用原生虚拟机管理软件的时间。原来我们新增一台虚拟机需要配置好内存、硬盘、CPU等，然后添加iso，安装。创建用户，等等。一套下来好几十分钟是吧？聪明点你可能会想到复制一个创建好的镜像然后粘贴。但这一切vagrant都帮你想好了,安装vagrant后你只需要6步就能创建一台新的虚拟机，其中两步是创建文件夹和切换文件夹。从安装到创建一台新的虚拟机就成功了。如果你想要再添加一台虚拟机，你只需要执行最后两步，添加一个不同名字的配置就能再新建一台虚拟机。还支持镜像、开机自动运行脚本、插件编写等。dockerdocker主要应用于解决环境依赖以及为应用程序提供一个相对隔离的空间，一个实例像操作系统里运行的一个程序。原来部署一套环境是不是得自己编写自动化部署依赖环境以及程序的脚本？如果有两个依赖同一程序或库的不同版本怎么办？绝对路径？软连接？docker能很好的解决你的烦恼。把需要的依赖环境打包成一个镜像，再把程序放镜像里面运行。 总的来说vagrant更适合给开发大爷们创造一个统一的开发、测试、接近于完全隔离的环境，以及提高对高配机的闲置利用。docker更方便地解决了同一机器上的环境隔离，以及提高运维锅们解决部署时环境依赖的效率。","tags":[{"name":"docker","slug":"docker","permalink":"http://jishusuishouji.github.io/tags/docker/"}]},{"title":"使用 Velocity 模板引擎快速生成代码","date":"2017-01-21T23:35:11.000Z","path":"2017/01/22/java/velocity/使用_Velocity_模板引擎快速生成代码/","text":"Velocity 模板引擎介绍在现今的软件开发过程中，软件开发人员将更多的精力投入在了重复的相似劳动中。特别是在如今特别流行的MVC架构模式中，软件各个层次的功能更加独立，同时代码的相似度也更加高。所以我们需要寻找一种来减少软件开发人员重复劳动的方法，让程序员将更多的精力放在业务逻辑以及其他更加具有创造力的工作上。Velocity这个模板引擎就可以在一定程度上解决这个问题。Velocity是一个基于Java的模板引擎框架，提供的模板语言可以使用在Java中定义的对象和变量上。Velocity是Apache基金会的项目，开发的目标是分离MVC模式中的持久化层和业务层。但是在实际应用过程中，Velocity不仅仅被用在了MVC的架构中，还可以被用在以下一些场景中。 1.Web应用：开发者在不使用JSP的情况下，可以用Velocity让HTML具有动态内容的特性。2.源代码生成：Velocity可以被用来生成Java代码、SQL或者PostScript。有很多开源和商业开发的软件是使用Velocity来开发的。3.自动Email：很多软件的用户注册、密码提醒或者报表都是使用Velocity来自动生成的。使用Velocity可以在文本文件里面生成邮件内容，而不是在Java代码中拼接字符串。4.转换xml：Velocity提供一个叫 Anakia 的ant任务，可以读取XML文件并让它能够被 Velocity模板读取。一个比较普遍的应用是将xdoc文档转换成带样式的HTML文件。 Hello Velocity和学习所有新的语言或者框架的顺序一样，我们从Hello Velocity开始学习。首先在Velocity的官网上下载最新的发布包，之后使用Eclipse建立普通的Java项目。引入解压包中的 velocity-1.7.jar和lib文件夹下面的jar包。这样我们就可以在项目中使用Velocity了。在做完上面的准备工作之后，就可以新建一个叫HelloVelocity的类，代码如下： 清单 1. HelloVelocity.java1234567891011121314151617181920212223242526public class HelloVelocity &#123; public static void main(String[] args) &#123; VelocityEngine ve = new VelocityEngine(); ve.setProperty(RuntimeConstants.RESOURCE_LOADER, &quot;classpath&quot;); ve.setProperty(&quot;classpath.resource.loader.class&quot;, ClasspathResourceLoader.class.getName()); ve.init(); Template t = ve.getTemplate(&quot;hellovelocity.vm&quot;); VelocityContext ctx = new VelocityContext(); ctx.put(&quot;name&quot;, &quot;velocity&quot;); ctx.put(&quot;date&quot;, (new Date()).toString()); List temp = new ArrayList(); temp.add(&quot;1&quot;); temp.add(&quot;2&quot;); ctx.put(&quot;list&quot;, temp); StringWriter sw = new StringWriter(); t.merge(ctx, sw); System.out.println(sw.toString()); &#125;&#125; 在HelloVelocity的代码中，首先new了一个VelocityEngine类，这个类设置了Velocity使用的一些配置，在初始化引擎之后就可以读取hellovelocity.vm这个模板生成的Template这个类。之后的VelocityContext类是配置Velocity模板读取的内容。这个context可以存入任意类型的对象或者变量，让template来读取。这个操作就像是在使用JSP开发时，往request里面放入key-value，让JSP 读取一样。接下来就是写hellovelocity.vm文件了，这个文件实际定义了Velocity的输出内容和格式。hellovelocity.vm的内容如下： 清单 2. Hellovelocity.vm1234567#set( $iAmVariable = &quot;good!&quot; )Welcome $name to velocity.comtoday is $date.#foreach ($i in $list)$i#end$iAmVariable 输出结果如下：Welcome velocity to velocity.comtoday is Sun Mar 23 19:19:04 CST 2014.12good!在输出结果中我们可以看到，$name、$date 都被替换成了在 HelloVelocity.java 里面定义的变量，在 foreach 语句里面遍历了 list 的每一个元素，并打印出来。而$iAmVariable 则是在页面中使用 #set 定义的变量。回页首基本模板语言语法使用在 hellovelocity.vm 里面可以看到很多以 # 和$符开头的内容，这些都是 Velocity 的语法。在 Velocity 中所有的关键字都是以 # 开头的，而所有的变量则是以$开头。Velocity 的语法类似于 JSP 中的 JSTL，甚至可以定义类似于函数的宏，下面来看看具体的语法规则。一、变量和我们所熟知的其他编程语言一样，Velocity 也可以在模板文件中有变量的概念。 变量定义#set($name =“velocity”)等号后面的字符串 Velocity 引擎将重新解析，例如出现以$开始的字符串时，将做变量的替换。#set($hello =“hello $name”)上面的这个等式将会给$hello 赋值为“hello velocity” 变量的使用在模板文件中使用$name 或者${name} 来使用定义的变量。推荐使用${name} 这种格式，因为在模板中同时可能定义了类似$name 和$names 的两个变量，如果不选用大括号的话，引擎就没有办法正确识别$names 这个变量。对于一个复杂对象类型的变量，例如$person，可以使用${person.name} 来访问 person 的 name 属性。值得注意的是，这里的${person.name} 并不是直接访问 person 的 name 属性，而是访问 person 的 getName() 方法，所以${person.name} 和${person.getName()} 是一样的。 变量赋值在第一小点中，定义了一个变量，同时给这个变量赋了值。对于 Velocity 来说，变量是弱数据类型的，可以在赋了一个 String 给变量之后再赋一个数字或者数组给它。可以将以下六种数据类型赋给一个 Velocity 变量：变量引用, 字面字符串, 属性引用, 方法引用, 字面数字, 数组列表。#set($foo = $bar)#set($foo =“hello”)#set($foo.name = $bar.name)#set($foo.name = $bar.getName($arg))#set($foo = 123)#set($foo = [“foo”,$bar])二、循环在 Velocity 中循环语句的语法结构如下：#foreach($element in $list)This is $element$velocityCount#endVelocity 引擎会将 list 中的值循环赋给 element 变量，同时会创建一个$velocityCount 的变量作为计数，从 1 开始，每次循环都会加 1.三、条件语句条件语句的语法如下#if(condition)…#elseif(condition)…#else…#end四、关系操作符Velocity 引擎提供了 AND、OR 和 NOT 操作符，分别对应&amp;&amp;、||和! 例如：#if($foo &amp;&amp; $bar)#end五、宏Velocity 中的宏可以理解为函数定义。定义的语法如下：#macro(macroName arg1 arg2 …)…#end调用这个宏的语法是：#macroName(arg1 arg2 …)这里的参数之间使用空格隔开，下面是定义和使用 Velocity 宏的例子：#macro(sayHello $name)hello $name#end#sayHello(“velocity”)输出的结果为 hello velocity六、#parse 和 #include#parse 和 #include 指令的功能都是在外部引用文件，而两者的区别是，#parse 会将引用的内容当成类似于源码文件，会将内容在引入的地方进行解析，#include 是将引入文件当成资源文件，会将引入内容原封不动地以文本输出。分别看以下例子：foo.vm 文件：#set($name =“velocity”)parse.vm：#parse(“foo.vm”)输出结果为：velocityinclude.vm：#include(“foo.vm”)输出结果为：#set($name =“velocity”)以上内容包含了部分 Velocity 的语法，详细的语法内容可以参考 Velocity 的官方文档。回页首自动生成代码的例子在上个例子中我们可以生成任意的字符串并且打印出来，那为什么我们不能生成一些按照既定格式定义的代码并且写入文件呢。在这里我们以一个实际的 demo 来完成这部分内容。相关内容的源码可以参照附件。这个 demo 的功能是要实现一个学生和老师的管理，实际上都是单张表的维护。我们希望能够只定义 model 层，来生成 MVC 的所有代码。在这个 demo 中，只自动生成 action 和 JSP 的内容，因为现在有很多工具都可以帮助我们自动生成这两个包的代码。首先在 eclipse 中建立一个 Java web 工程，在例子中为了方便管理 jar 包，使用的是 maven 来建立和管理工程。建立好的工程目录结构如下图所示：图 1. 项目目录结构项目目录结构Java Resource 中放的是 Java 源码以及资源文件，Deployed Resources 中放的是 web 相关的文件。在 Java 文件中使用了类似 Spring 的 @Component 和 @Autowired 的注解来实现 IoC，使用 @Action 这样的注解实现 MVC，而在 JSP 中则使用了 JSTL 来输出页面。在上图所示的目录中，annotation、filter、framework 和 util 这四个 package 是作为这个项目框架的，跟业务没有关系，类似于 spring 和 struts 的功能。在实际的项目中我们当然希望能够一开始就编写一个通用的模板文件，然后一下子生成所有的代码，但是很多时候这样做是不可能的，或者说比较困难。为了解决这个问题，我们可以在编写 Velocity 模板文件之前先按照原本的流程编写代码，暂时先忘掉 Velocity。编写的代码应该能够在一个功能上完整的调通涉及 MVC 中所有层次的内容。在这个例子中，先编写好 StudentAction.java 文件，以及上图中 webapp 目录中所示的文件。在写好以上代码，同时也能顺利运行之后，我们可以参照之前编写的代码来写模板文件。这里我们来分别看一个 Java 文件和 JSP 的例子。清单 3. ActionTemplate.vm#parse (“macro.vm”) @Action(“${classNameLowCase}Action”)public class ${classNameUpCase}Action extends BaseAction{ @Autowired public ${classNameUpCase}Dao ${classNameLowCase}Dao; private List&lt;${classNameUpCase}&gt; ${classNameLowCase}s; private ${classNameUpCase} ${classNameLowCase}; #foreach ($attr in ${attrs}) private ${attr[0]} ${attr[1]}; #end public String ${classNameLowCase}List() { ${classNameLowCase}s = ${classNameLowCase}Dao.retrieveAll${classNameUpCase}s(); return “${classNameLowCase}List.jsp”; } …}上面的代码展示了一个 Java 类转换成 vm 模板之后的部分内容，完整内容请参考附件。macro.vm 文件中定义了一些使用的宏。JSP 的改造相对于 Java 文件来说稍微有点复杂，因为 JSP 中使用 JSTL 取 request 中的值也是使用${name} 这样的语法，所以想要输出${name} 这样的字符串而不是被模板引擎所替换，则需要使用转义字符，就像这样：\\${name}。为了能够让这个文件中的 table 得到复用，我们将这个文件中的表格单独拿出来，使用 #parse 命令来包含。下面是 ListJspTemplate.vm 和 ListTableTemplate.vm 的内容：清单 4. ListJspTemplate.vm&lt;%@ page language=”java” contentType=”text/html; charset=UTF-8” pageEncoding=”UTF-8”%&gt;&lt;%@taglib prefix=”c” uri=”http://java.sun.com/jsp/jstl/core“ %&gt;&lt;!DOCTYPE html PUBLIC “-//W3C//DTD HTML 4.01 Transitional//EN” “http://www.w3.org/TR/html4/loose.dtd&quot;&gt; &lt;%@ include file=”includeJS.jsp” %&gt; var pageConfig = { “list” : { “action” : “${classNameLowCase}Action!${classNameLowCase}List.action” } … “idName” : “${classNameLowCase}Id” }; ${classNameUpCase} List ${classNameUpCase} List Add #parse (“ListTableTemplate.vm”) 清单 5. ListTableTemplate.vm #parse (“macro.vm”) #set($plus = “status.index+1”) No.#generateTH($attrs) ${${plus}}#generateTD($classNameLowCase $attrs) Modify Delete 在定义好所有的模板文件之后，需要做的是读取这些文件，然后根据这些文件将 model 的数据类型以及名称设置到 context 中，最后将解析出来的内容写到相应的目录中去。这些工作我们放在了一个叫做 VelocityGenerator 的类中来做，它的源码如下：清单 6. TemplateGenerator.javapublic class VelocityGenerator { public static void main(String[] args) { VelocityEngine ve = new VelocityEngine(); ve.setProperty(RuntimeConstants.RESOURCE_LOADER, “classpath”); ve.setProperty(“classpath.resource.loader.class”, ClasspathResourceLoader.class.getName()); ve.init(); Template actionTpt = ve.getTemplate(“ActionTemplate.vm”); Template listJspTpt = ve.getTemplate(“ListJspTemplate.vm”); Template addTpt = ve.getTemplate(“AddTemplate.vm”); Template modifyTpt = ve.getTemplate(“ModifyTemplate.vm”); VelocityContext ctx = new VelocityContext(); ctx.put(“classNameLowCase”, “teacher”); ctx.put(“classNameUpCase”, “Teacher”); String[][] attrs = { {“Integer”,”id”}, {“String”,”name”}, {“String”,”serializeNo”}, {“String”,”titile”}, {“String”,”subject”} }; ctx.put(“attrs”, attrs); String rootPath = VelocityGenerator.class.getClassLoader().getResource(“”).getFile() + “../../src/main”; merge(actionTpt,ctx,rootPath+”/java/com/liuxiang/velocity/action/TeacherAction.java”); merge(listJspTpt,ctx,rootPath+”/webapp/teacherList.jsp”); merge(addTpt,ctx,rootPath+”/webapp/teacherAdd.jsp”); merge(modifyTpt,ctx,rootPath+”/webapp/teacherModify.jsp”); System.out.println(“success…”); } private static void merge(Template template, VelocityContext ctx, String path) { PrintWriter writer = null; try { writer = new PrintWriter(path); template.merge(ctx, writer); writer.flush(); } catch (FileNotFoundException e) { e.printStackTrace(); } finally { writer.close(); } }}在运行以上代码之后，项目文件夹中将会出现与 Teacher 相关的代码文件。在实际项目中可能不会出现很多这种单张表维护的情况，而且业务逻辑和系统架构会更加复杂，编写模板文件就更加不容易。但是无论多复杂的系统，不同的业务逻辑之间一定或多或少会有相似的代码，特别是在 JSP 和 JS 显示端文件中，因为我们在一个系统中要求显示风格、操作方式一致的时候就免不了会有相似内容的代码出现。在总结这些相似性之后我们还是可以使用 Velocity 来帮助我们生成部分内容的代码，而且即使有一些非共性的内容，我们也可以在生成的代码中继续修改。使用 Velocity 的另外一个好处是生成出来的代码更好维护，风格更加统一。回页首结束语Velocity 可以被应用在各种各样的情景下，本文介绍的只是它的一种用途而已，它还可以被用来做 MVC 结构中的 view 层，或者动态内容静态化等。另外，Velocity 并不是唯一的模板框架，同样很优秀的 Freemarker 也获得了非常广泛的应用，有兴趣的读者可以去深入研究更多的功能和用途。","tags":[{"name":"Velocity","slug":"Velocity","permalink":"http://jishusuishouji.github.io/tags/Velocity/"}]},{"title":"Spring中提示元素 'ref' 中不允许出现属性 'local'","date":"2017-01-17T07:58:44.000Z","path":"2017/01/17/spring/Spring中提示元素__ref__中不允许出现属性__local_/","text":"这个问题在Spring4.X以前的版本不存在。通过查询Spring的官方文档Spring4.X的以上版本不支持该属性了。下面是官方说明： The local attribute on the ref element is no longer supported in the 4.0 beans xsd since it does not provide value over a regular bean reference anymore. Simply change your existing ref local references to ref bean when upgrading to the 4.0 schema. 官方建议使用bean在Spring4.0以上的版本。 案例重现抛错：1234567891011121314151617181920212223242526272829303132333435363738394041Exception in thread &quot;main&quot; org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 37 in XML document from class path resource [application_dependencies.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 37; columnNumber: 27; cvc-complex-type.3.2.2: 元素 &apos;ref&apos; 中不允许出现属性 &apos;local&apos;。 at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399) at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336) at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304) at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181) at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217) at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188) at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:252) at org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:127) at org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:93) at org.springframework.context.support.AbstractRefreshableApplicationContext.refreshBeanFactory(AbstractRefreshableApplicationContext.java:129) at org.springframework.context.support.AbstractApplicationContext.obtainFreshBeanFactory(AbstractApplicationContext.java:604) at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:509) at org.springframework.context.support.ClassPathXmlApplicationContext.&lt;init&gt;(ClassPathXmlApplicationContext.java:139) at org.springframework.context.support.ClassPathXmlApplicationContext.&lt;init&gt;(ClassPathXmlApplicationContext.java:83) at com.mxsm.spring.SpringDependencies.main(SpringDependencies.java:64)Caused by: org.xml.sax.SAXParseException; lineNumber: 37; columnNumber: 27; cvc-complex-type.3.2.2: 元素 &apos;ref&apos; 中不允许出现属性 &apos;local&apos;。 at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198) at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325) at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:453) at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3232) at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2709) at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2051) at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:761) at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:353) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2717) at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:607) at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:116) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:489) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:835) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:764) at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:123) at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:237) at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:300) at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76) at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429) at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391) ... 14 more spring xml文件配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;bean id=&quot;animals&quot; class=&quot;com.mxsm.spring.bean.Animal&quot;&gt; &lt;constructor-arg&gt; &lt;ref bean=&quot;dog&quot;/&gt; &lt;/constructor-arg&gt; &lt;constructor-arg&gt; &lt;ref bean=&quot;cat&quot;/&gt; &lt;/constructor-arg&gt; &lt;/bean&gt; &lt;!--使用type属性--&gt; &lt;bean id=&quot;dog&quot; class=&quot;com.mxsm.spring.bean.Dog&quot;&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;aa&quot;/&gt; &lt;constructor-arg type=&quot;int&quot; value=&quot;1&quot;/&gt; &lt;constructor-arg type=&quot;java.lang.String&quot; value=&quot;meat&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;dog_2&quot; class=&quot;com.mxsm.spring.bean.Dog&quot;&gt; &lt;constructor-arg index=&quot;0&quot; value=&quot;ssss&quot;/&gt; &lt;constructor-arg index=&quot;1&quot; value=&quot;3333&quot;/&gt; &lt;constructor-arg index=&quot;2&quot; value=&quot;8888&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;cat&quot; class=&quot;com.mxsm.spring.bean.Cat&quot;&gt; &lt;constructor-arg name=&quot;a&quot; value=&quot;ssss&quot;/&gt; &lt;constructor-arg name=&quot;b&quot; value=&quot;3333&quot;/&gt; &lt;/bean&gt; &lt;!-- setter 依赖注入bean --&gt; **&lt;bean id=&quot;man&quot; class=&quot;com.mxsm.spring.bean.Man&quot;&gt; &lt;property name=&quot;white&quot;&gt; &lt;ref local=&quot;whiteMan&quot;/&gt; &lt;/property&gt; &lt;property name=&quot;yellow&quot;&gt; &lt;ref local =&quot;yellowMan&quot;/&gt; &lt;/property&gt; &lt;property name=&quot;id&quot; value=&quot;1&quot;/&gt; &lt;/bean&gt;** &lt;bean id=&quot;whiteMan&quot; class=&quot;com.mxsm.spring.bean.WhitePerson&quot;&gt; &lt;constructor-arg name=&quot;age&quot; value=&quot;1&quot;/&gt; &lt;constructor-arg name=&quot;color&quot; value=&quot;white&quot;/&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;USA&quot;/&gt; &lt;constructor-arg name=&quot;sex&quot; value=&quot;男&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;yellowMan&quot; class=&quot;com.mxsm.spring.bean.YellowPerson&quot;&gt; &lt;constructor-arg name=&quot;age&quot; value=&quot;1&quot;/&gt; &lt;constructor-arg name=&quot;color&quot; value=&quot;yellow&quot;/&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;China&quot;/&gt; &lt;constructor-arg name=&quot;sex&quot; value=&quot;男&quot;/&gt; &lt;/bean&gt;&lt;/beans&gt;","tags":[{"name":"spring","slug":"spring","permalink":"http://jishusuishouji.github.io/tags/spring/"}]},{"title":"nodejs的require模块及路径","date":"2017-01-13T17:30:53.000Z","path":"2017/01/14/nodejs/nodejs的require模块及路径/","text":"在nodejs中，模块分为核心模块和文件模块。 核心模块是被编译成二进制代码，引用的时候只需require即可，如require(&#39;net&#39;)。文件模块，则是指js文件、json文件或者是.node文件。在引用文件模块的时候要加上文件的路径：如果既不加/.../...、../又不加./的话，则该模块要么是核心模块，要么是从一个node_modules文件夹加载。 如果’/home/ry/projects/foo.js‘ 中的文件调用了`require(‘bar.js’)`` ，node将在下面的位置进行搜索： •/home/ry/projects/node_modules/bar.js•/home/ry/node_modules/bar.js•/home/node_modules/bar.js•/node_modules/bar.js 文件夹作为模块：首先在./some-library文件夹下建立package.json文件，它标识了一个主模块。一个package.json中的内容可能如下：1234&#123; &quot;name&quot; : &quot;some-library&quot;, &quot;main&quot; : &quot;./lib/some-library.js&quot; &#125; require(&#39;./some-library&#39;)(和some-library相同路径的js文件)时将试图加载./some-library/lib/some-library.js如果在这个目录下没有package.json文件，node将试图从这个目录下加载index.js或index.node文件。例如，如果上面没有package.json文件，那么require(&#39;./some-library&#39;)时，将试图加载下面的文件：•./some-library/index.js•./some-library/index.node 分类: javascript,nodejs标签: javascript, nodejs","tags":[{"name":"nodejs","slug":"nodejs","permalink":"http://jishusuishouji.github.io/tags/nodejs/"}]},{"title":"Spring MVC事务配置","date":"2017-01-09T14:36:01.000Z","path":"2017/01/09/java/spring/Spring_MVC事务配置/","text":"","tags":[]},{"title":"是该抛弃Spring HibernateTemplate的时候了","date":"2017-01-09T14:26:48.000Z","path":"2017/01/09/java/spring/是该抛弃Spring_HibernateTemplate的时候了/","text":"在spring2.0之前，我们在使用hibernate和spring的时候，都会被HibernateTemplate为我们提供benefits（资源和事务管理以及把那个“丑陋”的checked exception转换为runtime exception-DataAccessException ）而折服，在项目中不由自主、不假思索地使用它和那个经典的callback方法。而如今，hibernate3.0.1+ 、spring 2.0+版本以后，我们可以在数据访问层直接使用hinberate的session API(例如SessionFactory.getCurrentSession)，不并担心session和transaction management。至于error handling可以通过spring的@Repository annotation和post processor-PersistenceExceptionTranslationPostProcessor来解决。让我们来看一些代码片段：123456789&lt;bean id=&quot;sessionFactory&quot; class=&quot;org.springframework.orm.hibernate3. LocalSessionFactoryBean&quot;&gt; &lt;!-- the properties setting--&gt; &lt;/bean&gt; &lt;bean id=&quot;accountRepo&quot; class=&quot;com.mycompany.HibernateAccountRepository&quot;&gt; &lt;constructor-arg ref=&quot;sessionFactory&quot;&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.dao.annotation. PersistenceExceptionTranslationPostProcessor&quot;/&gt; 数据访问层代码片段：123456789101112131415@Repository public class HibernateAccountRepository implements AccountRepository &#123; private SessionFactory factory; public HibernateAccountRepository(SessionFactory factory) &#123; this.factory = factory; &#125; public Account loadAccount(String username) &#123; return (Account)factory.getCurrentSession() .createQuery(&quot;from Account acc where acc.name = :name&quot;) .setParameter(&quot;name&quot;, &quot;thethirdpart&quot;).uniqueResult(); &#125; &#125; 在xml配置文件里面通过配置的post processor会自动检测@Repository标注的bean并为该bean打开exception转换功能。 如果不支持annotations，可以通过AOP来实现，更方便123456&lt;bean id=&quot;persistenceExceptionInterceptor&quot; class=&quot;org.springframework.dao.support.PersistenceExceptionTranslationInterceptor&quot;/&gt; &lt;aop:config&gt; &lt;aop:advisor pointcut=&quot;execution(* *..*Repository+.*(..))&quot; advice-ref=&quot;persistenceExceptionInterceptor&quot; /&gt; &lt;/aop:config&gt; 总结，我们应该选择哪种方式呢？还是那句话，根据不同的情况来做最正确的选择。但我建议是丢弃template，而直接使用hibernate的API，毕竟灵活性更大，更何况遇到复杂的情况我们始终得面对hibernate的API。spring并不强制你做任何事情，记得它是一个非侵入性的framework。","tags":[{"name":"spring","slug":"spring","permalink":"http://jishusuishouji.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://jishusuishouji.github.io/tags/java/"}]},{"title":"So should you still use Spring's HibernateTemplate and/or JpaTemplate??","date":"2017-01-09T14:20:51.000Z","path":"2017/01/09/java/spring/So_should_you_still_use_Spring_s_HibernateTemplate_and_or_JpaTemplate__/","text":"I was reading an article by Vigil Bose on TSS the other day and saw the usage of the HibernateDaoSupport class. Since this is no longer a recommended way of using Hibernate from Spring, I thought I might as well just blog about it another time. 不建议使用HibernateDaoSupport。 With the advent(n. 到来；出现；) of Spring 2.0, it has become possible to start using the Hibernate Session API directly again. The question is whether or not it is wise to abandon the use of the HibernateTemplate when working with Hibernate, or any other template-based approaches Spring features. Using Spring XxxTemplatesIn Spring 1.0, we introduced a revolutionary way of working with data access APIs that threw checked exceptions. The template approach Spring features along with its transaction synchronization manager and the extensive(adj. 广泛的；大量的；广阔的) use of runtime exceptions makes any TCFTC (short for try/catch-finally-try/catch as we coined(杜撰) it back in 2005) often found in data access code entirely obsolete. Below you can see (a simplified version and not entirely precise version of) what Spring’s template approach does for you (with specific code snippets that you would otherwise have to write). Acquisition of connection: If transaction synchronization is active (which it is, if you’re using Spring’s transaction management infrastructure), most of the times any of the Spring templates are using the same connection across the entire thread (things are actually a bit more complicated than that, but that would lead us too much into the gory details). Participation in a transaction Again, when using transaction management features, Spring will automatically associated any new connection with the current transaction. This again, all depends on the current propagations settings and so on, but whichever way you look at it, your core code is not affected by it. Specification of the SQL: This is what you (obviously) have to do yourself. The SQL ideally uses bind parameters, to avoid any chances of SQL injection from happening. Parameters are passed to the JDBC template as arguments. Creation / execution of statement and iterating over result set: After you’ve specified the SQL, Spring is going to create the statement for you, set any parameters you may have specified, execute it and loop over the result set for you. Parse result from result set: You can opt for parsing the result set yourself if you like (or if you have complex parsing requirements), or you can have Spring result a list of primitives, or just one value from the result set. Handling and translation of exceptions: This is where Spring translates any exceptions that might have occurred to Spring’s own DataAccessException hierarchy, automatically insulating calling code from the data access technology in use. Releasing of connection: This is the last piece of the puzzle where Spring releases any resources used. Of course, if transaction synchronization is active, the resources might not be released immediately. Templates are available for several APIs such as: JDBC (JdbcTemplate) Hibernate (HibernateTemplate) iBatis (SqlMapClientTemplate) JDO (JdoTemplate) TopLink (TopLinkTemplate) Messaging (JmsTempate) Transaction management (TransactionTemplate) JNDI (JndiTemplate) Are templates really necessary?The templates add a lot of value when using an API that uses checked exceptions (as opposed to runtime exceptions or unchecked exceptions), but also add a lot of consistency to your code base. People having learnt Spring’s JdbcTemplate can pretty easily start using Spring’s JdoTemplate or Spring’s HibernateTemplate–the approach to using those is similar for each one of them. The most visible impact of the Spring template approach is the code reduction for for example JDBC. This is primarily because the checked exceptions are translated to runtime exceptions inside the template, removing the need to catch the exception in your mainline code. Other reasons are the transparent resource management and automatic synchronization with the currently running transaction. Of course it’s fairly easy to change a framework to use runtime exceptions natively instead of Spring having to do this and this is what for example Hibernate has started to do from version 3.0 onwards. Hibernate is not the only technology to do this–the Java Persistence API is also using runtime exceptions.","tags":[{"name":"hibernate","slug":"hibernate","permalink":"http://jishusuishouji.github.io/tags/hibernate/"},{"name":"spring","slug":"spring","permalink":"http://jishusuishouji.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://jishusuishouji.github.io/tags/java/"},{"name":"HibernateTemplate","slug":"HibernateTemplate","permalink":"http://jishusuishouji.github.io/tags/HibernateTemplate/"}]},{"title":"java分布式事务(JTA)实现 jotm和atomikos","date":"2017-01-08T02:03:52.000Z","path":"2017/01/08/java/jta/java分布式事务_JTA_实现 jotm和atomikos/","text":"本地事务：只对单一数据源(单个数据库)事务进行控制。分布式事务：处理多种异构的数据源， 比如某个业务操作中同时包含JDBC和JMS或者某个操作需要访问多个不同的数据库，在不同数据库之间进行事务控制。 在Java中，分布式事务主要的规范是JTA/XA。其中：JTA是Java的事务管理器规范，XA是工业标准的X/Open CAE规范，可被两阶段提交及回滚的事务资源定义。比如某数据库实现了XA规范，则不管是JTA，还是MSDTC，都可以基于同样的行为对该数据库进行事务处理。 JTA全称为Java Transaction API，顾名思义JTA定义了一组统一的事务编程的接口，这些接口如下： XAResource：XAResource接口是对实现了X/Open CAE规范的资源管理器 (Resource Manager，数据库就是典型的资源管理器) 的抽象，它由资源适配器 (Resource Apdater) 提供实现。XAResource是支持事务控制的核心。Transaction：Transaction接口是一个事务实例的抽象，通过它可以控制事务内多个资源的提交或者回滚。二阶段提交过程也是由Transaction接口的实现者来完成的。TransactionManager：托管模式 (managed mode) 下，TransactionManager接口是被应用服务器调用，以控制事务的边界的。UserTransaction：非托管模式 (non-managed mode) 下，应用程序可以通过UserTransaction接口控制事务的边界 在tomcat下是没有分布式事务的，可以借助于第三方Jotm和Automikos实现，在spring中分布式事务是通过jta（jotm，atomikos）来进行实现。即：通过代码的方式来决定是否是分布式事务。 注：推荐用服务器自己的数据源(也就是 lookup JNDI)，这样的话，是不是XA事务就由服务器的配置来定制，代码就不需要任何配置来决定是不是XA了。事务本身是不是XA (分布式的）是服务器的事，服务器来管理“资源” （包括数据源，JMS 连接等，一个资源（JDBC连接）如何参与事务是“资源管理器”（驱动程序）的职责，跟程序无关），服务器提供事务管理并作为“事务协调者”来处理多个“资源管理器”（不同的数据库连接）之间的事务一致性。 jotm和automikos网址：1、http://jotm.objectweb.org/2、http://www.atomikos.com/Main/TransactionsEssentials Spring 通过AOP技术可以让我们在脱离EJB的情况下享受声明式事务的丰盛大餐。此外，通过配合使用ObjectWeb的JOTM开源项目，不需要Java EE应用服务器，Spring也可以提供JTA事务。 正因为AOP让Spring拥有了脱离EJB容器的声明式事务能力，而JOTM让我们在脱离Java EE应用服务器下拥有JTA事务能力。所以，人们将AOP和JOTM称为Java软件开发的两个圣杯。 JTA的实现框架有：GeronimoTM/Jencks 官方文档比较少，不适合学习和维护。SimpleJTA 没有实现JTS (Java Transaction Service)而且不是活跃的。Atomikos 是一个另人钦佩的产品。有丰富的文档，而且有很好的支持。JBossTS 是一个应用在JBOSS服务器上的，肯定是一个成熟的产品，也有好的支持，详细信息可以看这里：http://www.theserverside.com/news/thread.tss?thread_id=37941最常见的二个如下：JOTM JOTM(Java Open Transaction Manager)是ObjectWeb的一个开源JTA实现，它本身也是开源应用程序服务器JOnAS(Java Open Application Server)的一部分，为其提供JTA分布式事务的功能。 存在的问题：使用中不能自动rollback，无论什么情况都commit。注：spring3开始已经不再支持jotm Atomikos 大家推荐最多的。和JOTM相比Atomikos Transactions Essentials更加稳定，它原来是商业项目，现在开源了。象MySQL一样卖服务支持的。而且论坛页比较活跃，有问题很快可以解决。","tags":[{"name":"java","slug":"java","permalink":"http://jishusuishouji.github.io/tags/java/"},{"name":"jta","slug":"jta","permalink":"http://jishusuishouji.github.io/tags/jta/"},{"name":"分布式事务","slug":"分布式事务","permalink":"http://jishusuishouji.github.io/tags/分布式事务/"},{"name":"jotm","slug":"jotm","permalink":"http://jishusuishouji.github.io/tags/jotm/"},{"name":"atomikos","slug":"atomikos","permalink":"http://jishusuishouji.github.io/tags/atomikos/"}]},{"title":"java分布式事务:spring+JTA+jotm","date":"2017-01-08T01:23:24.000Z","path":"2017/01/08/java/jta/java分布式事务_spring_JTA_jotm/","text":"业务背景当新建用户时需插入一条用户记录，同时还需在另一个DB中记录日志。因为是不同的DB操作，所以及到分布式事务的处理。 1、代码结构： 2、建表语句：1234567create database log; DROP TABLE IF EXISTS `log`; CREATE TABLE `log` ( `id` varchar(20) NOT NULL, `content` varchar(100) default NULL, PRIMARY KEY (`id`) ); 1234567create database user; DROP TABLE IF EXISTS `user`; CREATE TABLE `user` ( `id` varchar(20) NOT NULL, `name` varchar(40) default NULL, PRIMARY KEY (`id`) ); 3、配置文件application.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273&lt;!--?xml version=1.0 encoding=UTF-8?--&gt; &lt;beans aop=&quot;&quot; beans=&quot;&quot; http:=&quot;&quot; schema=&quot;&quot; spring-aop.xsd=&quot;&quot; spring-beans.xsd=&quot;&quot; spring-tx.xsd=&quot;&quot; tx=&quot;&quot; www.springframework.org=&quot;&quot; xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot; xmlns:tx=&quot;http://www.springframework.org/schema/tx&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemalocation=&quot;http://www.springframework.org/schema/beans&quot;&gt; &lt;!-- 引用Spring内部所提供的对JOTM支持的工厂类 --&gt; &lt;bean class=&quot;org.springframework.transaction.jta.JotmFactoryBean&quot; id=&quot;jotm&quot;/&gt; &lt;!-- 配置JTA事务管理器, 并在管理器中使用上面所配置的JOTM --&gt; &lt;bean class=&quot;org.springframework.transaction.jta.JtaTransactionManager&quot; id=&quot;txManager&quot;&gt; &lt;property name=&quot;userTransaction&quot; ref=&quot;jotm&quot;/&gt; &lt;/bean&gt; &lt;!-- 配置多个数据源 --&gt; &lt;bean class=&quot;org.enhydra.jdbc.pool.StandardXAPoolDataSource&quot; destroy-method=&quot;shutdown&quot; id=&quot;db1&quot;&gt; &lt;property name=&quot;dataSource&quot;&gt; &lt;bean class=&quot;org.enhydra.jdbc.standard.StandardXADataSource&quot; destroy-method=&quot;shutdown&quot;&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;jotm&quot;/&gt; &lt;property name=&quot;driverName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:MySQL://localhost:3306/user&quot;/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;org.enhydra.jdbc.pool.StandardXAPoolDataSource&quot; destroy-method=&quot;shutdown&quot; id=&quot;db2&quot;&gt; &lt;property name=&quot;dataSource&quot;&gt; &lt;bean class=&quot;org.enhydra.jdbc.standard.StandardXADataSource&quot; destroy-method=&quot;shutdown&quot;&gt; &lt;property name=&quot;transactionManager&quot; ref=&quot;jotm&quot;/&gt; &lt;property name=&quot;driverName&quot; value=&quot;com.mysql.jdbc.Driver&quot;/&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:MySQL://localhost:3306/log&quot;/&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;user&quot; value=&quot;root&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;root&quot;/&gt; &lt;/bean&gt; &lt;!-- 根据不同的数据源配置两个jdbcTemplate --&gt; &lt;bean class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot; id=&quot;jdbcTemplate1&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;db1&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;org.springframework.jdbc.core.JdbcTemplate&quot; id=&quot;jdbcTemplate2&quot;&gt; &lt;property name=&quot;dataSource&quot; ref=&quot;db2&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;com.zdp.dao.UserDao&quot; id=&quot;userDao&quot;&gt; &lt;property name=&quot;jdbcTemplate&quot; ref=&quot;jdbcTemplate1&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;com.zdp.dao.LogDao&quot; id=&quot;logDao&quot;&gt; &lt;property name=&quot;jdbcTemplate&quot; ref=&quot;jdbcTemplate2&quot;/&gt; &lt;/bean&gt; &lt;bean class=&quot;com.zdp.service.UserService&quot; id=&quot;userService&quot;/&gt; &lt;property name=&quot;userDao&quot; ref=&quot;userDao&quot;/&gt; &lt;property name=&quot;logDao&quot; ref=&quot;logDao&quot;/&gt; &lt;/bean&gt; &lt;!-- JTA事务传播特性 --&gt; &lt;tx:advice id=&quot;txAdviceJTA&quot; transaction-manager=&quot;txManager&quot;&gt; &lt;tx:attributes&gt; &lt;tx:method isolation=&quot;DEFAULT&quot; name=&quot;save*&quot; propagation=&quot;REQUIRED&quot; rollback-for=&quot;Exception/&quot;&gt; &lt;tx:method isolation=&quot;DEFAULT&quot; name=&quot;add*&quot; propagation=&quot;REQUIRED&quot; rollback-for=&quot;Exception/&quot;&gt; &lt;tx:method isolation=&quot;DEFAULT&quot; name=&quot;create*&quot; propagation=&quot;REQUIRED&quot; rollback-for=&quot;Exception/&quot;&gt; &lt;tx:method isolation=&quot;DEFAULT&quot; name=&quot;insert*&quot; propagation=&quot;REQUIRED&quot; rollback-for=&quot;Exception/&quot;&gt; &lt;tx:method isolation=&quot;DEFAULT&quot; name=&quot;del*&quot; propagation=&quot;REQUIRED&quot; rollback-for=&quot;Exception/&quot;&gt; &lt;tx:method isolation=&quot;DEFAULT&quot; name=&quot;update*&quot; propagation=&quot;REQUIRED&quot; rollback-for=&quot;Exception/&quot;&gt; &lt;tx:method name=&quot;*&quot; read-only=&quot;true/&quot;&gt; &lt;/tx:attributes&gt; &lt;/tx:advice&gt; &lt;/beans&gt; 4、service业务类：1234567891011121314151617181920212223242526public class UserService &#123; private UserDao userDao; private LogDao logDao; public void saveUser(String id, String name) &#123; userDao.insertUser(id, name); // int i = 1 / 0; // 制造异常 logDao.insertLog(id, id + _ + name); &#125; public UserDao getUserDao() &#123; return userDao; &#125; public void setUserDao(UserDao userDao) &#123; this.userDao = userDao; &#125; public LogDao getLogDao() &#123; return logDao; &#125; public void setLogDao(LogDao logDao) &#123; this.logDao = logDao; &#125; &#125; 5、dao类：123456public class UserDao extends JdbcDaoSupport &#123; public void insertUser(String id, String name) &#123; JdbcTemplate template = getJdbcTemplate(); template.execute(insert into user values(&apos; + id + &apos;,&apos; + name + &apos;)); &#125; &#125; 123456public class LogDao extends JdbcDaoSupport &#123; public void insertLog(String id, String content) &#123; JdbcTemplate template = getJdbcTemplate(); template.execute(insert into log values(&apos; + id + &apos;,&apos; + content + &apos;)); &#125; &#125; 6、测试类：12345678public class UserTest &#123; @Test public void testSave() &#123; ApplicationContext cxt = new ClassPathXmlApplicationContext(ApplicationContext.xml); UserService us = (UserService) cxt.getBean(userService); us.saveUser(1, zhangsan); &#125; &#125;","tags":[{"name":"spring","slug":"spring","permalink":"http://jishusuishouji.github.io/tags/spring/"},{"name":"java","slug":"java","permalink":"http://jishusuishouji.github.io/tags/java/"},{"name":"jta","slug":"jta","permalink":"http://jishusuishouji.github.io/tags/jta/"},{"name":"jotm","slug":"jotm","permalink":"http://jishusuishouji.github.io/tags/jotm/"}]}]